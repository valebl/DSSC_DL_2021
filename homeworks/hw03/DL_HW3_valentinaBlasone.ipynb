{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 03 - Valentina Blasone\r\n",
    "\r\n",
    "## Deep Learning - A.A. 2020/2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 1. Implement L1 norm regularization as a custom loss function\r\n",
    "> 2. Implement early stopping in the $E_{\\text{opt}}$ specification\r\n",
    "> 3. Implement early stopping in one of the additional specifications as of [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import os\r\n",
    "from torch import nn\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "from scripts import mnist\r\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1\r\n",
    "\r\n",
    "The L1 norm regularization can be implemented as a custom loss function, that takes in input a given loss function to be regularized and the model, from which the weights values are derived."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def l1_regularization(loss_values, learn_rate, model):\r\n",
    "    '''Custom loss functions that adds the L1\r\n",
    "    regularization to a given loss functions'''\r\n",
    "    l1 = sum(p.abs().sum() for name, p in model.named_parameters() if 'weight' in name)\r\n",
    "    loss = loss_values + learn_rate * l1\r\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the example of LAB 3 to check the correctness of the L1 norm regularization just implemented."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class MLP(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.flat = nn.Flatten()\r\n",
    "        self.h1 = nn.Linear(28*28, 16)\r\n",
    "        self.h2 = nn.Linear(16, 32)\r\n",
    "        self.h3 = nn.Linear(32, 24)\r\n",
    "        self.out = nn.Linear(24, 10)\r\n",
    "    \r\n",
    "    def forward(self, X, activ_hidden=nn.functional.relu):\r\n",
    "        out = self.flat(X)\r\n",
    "        out = activ_hidden(self.h1(out))\r\n",
    "        out = activ_hidden(self.h2(out))\r\n",
    "        out = activ_hidden(self.h3(out))\r\n",
    "        out = self.out(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, l1_rate=None): # note: I've added a generic performance to replace accuracy and the device\r\n",
    "    for X, y in dataloader:\r\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\r\n",
    "        X = X.to(device)\r\n",
    "        y = y.to(device)\r\n",
    "        # ... like last time\r\n",
    "        optimizer.zero_grad() \r\n",
    "        y_hat = model(X)\r\n",
    "        if l1_rate is not None:\r\n",
    "            loss = l1_regularization(loss_fn(y_hat,y), l1_rate, model)\r\n",
    "        else:\r\n",
    "            loss = loss_fn(y_hat,y)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        acc = performance(y_hat, y)\r\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\r\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\r\n",
    "\r\n",
    "\r\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, l1_rate=None, performance=accuracy, device=None): # note: I've added a generic performance to replace accuracy and an object where to store the trajectory and the device on which to run our training\r\n",
    "\r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "    print(f\"Training on {device}\")\r\n",
    "\r\n",
    "    if l1_rate is not None:\r\n",
    "        print(\"Training with L1 regularization\")\r\n",
    "    \r\n",
    "    model.to(device)\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # epoch loop\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "        performance_meter = AverageMeter()\r\n",
    "\r\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, l1_rate)\r\n",
    "\r\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\r\n",
    "\r\n",
    "    return loss_meter.sum, performance_meter.avg\r\n",
    "\r\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "\r\n",
    "    # create an AverageMeter for the loss if passed\r\n",
    "    if loss_fn is not None:\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "    \r\n",
    "    performance_meter = AverageMeter()\r\n",
    "\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            X = X.to(device)\r\n",
    "            y = y.to(device)\r\n",
    "\r\n",
    "            y_hat = model(X)\r\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\r\n",
    "            acc = performance(y_hat, y)\r\n",
    "            if loss_fn is not None:\r\n",
    "                loss_meter.update(loss.item(), X.shape[0])\r\n",
    "            performance_meter.update(acc, X.shape[0])\r\n",
    "    # get final performances\r\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\r\n",
    "    fin_perf = performance_meter.avg\r\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\r\n",
    "    return fin_loss, fin_perf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "minibatch_size_train = 256\r\n",
    "minibatch_size_test = 512\r\n",
    "\r\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)\r\n",
    "\r\n",
    "learn_rate = 0.1\r\n",
    "num_epochs = 10\r\n",
    "\r\n",
    "l1_rate = 1e-05\r\n",
    "\r\n",
    "model = MLP()\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_loss, train_acc = train_model(model, trainloader, loss_fn, optimizer, num_epochs, l1_rate, device=\"cuda:0\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cuda:0\n",
      "Training with L1 regularization\n",
      "Epoch 1 completed. Loss - total: 80186.87865447998 - average: 1.3364479775746663; Performance: 0.5613666666666667\n",
      "Epoch 2 completed. Loss - total: 25101.48248386383 - average: 0.41835804139773053; Performance: 0.8748166666666667\n",
      "Epoch 3 completed. Loss - total: 19658.83703660965 - average: 0.3276472839434942; Performance: 0.9042\n",
      "Epoch 4 completed. Loss - total: 16609.475224494934 - average: 0.2768245870749156; Performance: 0.9192333333333333\n",
      "Epoch 5 completed. Loss - total: 14976.65601181984 - average: 0.24961093353033065; Performance: 0.9262333333333334\n",
      "Epoch 6 completed. Loss - total: 13650.245250701904 - average: 0.2275040875116984; Performance: 0.9342666666666667\n",
      "Epoch 7 completed. Loss - total: 12588.35791015625 - average: 0.20980596516927083; Performance: 0.9389666666666666\n",
      "Epoch 8 completed. Loss - total: 11860.14377784729 - average: 0.1976690629641215; Performance: 0.9423166666666667\n",
      "Epoch 9 completed. Loss - total: 10802.1873087883 - average: 0.18003645514647165; Performance: 0.9478666666666666\n",
      "Epoch 10 completed. Loss - total: 10680.61698961258 - average: 0.17801028316020964; Performance: 0.94815\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "final_loss, final_perf = test_model(model, testloader, loss_fn=loss_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 11253.26155424118 - performance 0.9414833333333333\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. a.\r\n",
    "\r\n",
    "We define a MLP architecture in which the early stopping in the $E_{opt}$ specification is implemented. The idea is to stop the training as soon as the error on the validation set is higher than it was the last time it was checked.\r\n",
    "\r\n",
    "The value $E_{opt}(t)$ is defined to be the lowest validation set error obtained in epochs up to $t$: $$E_{opt}(t) := \\min_{t'\\leq t} E_{va}(t')$$\r\n",
    "\r\n",
    "Where $E_{va}$ is the validation error. We can then define the _generalization loss_ at epoch $t$ to be the relative increase of the validation error over the minimum-so-far in percent: $$GL(t)=100\\cdot \\left( \\dfrac{E_{va}(t)}{E_{opt}(t)}-1\\right)$$\r\n",
    "\r\n",
    "In this case, the stopping criterion says to stop as soon as the generalization loss exceeds a certain threshold, $\\alpha$: $$GL(t)>\\alpha$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, alpha=None, l1_rate=None, performance=accuracy, device=None): # note: I've added a generic performance to replace accuracy and an object where to store the trajectory and the device on which to run our training\r\n",
    "\r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "    print(f\"Training on {device}\")\r\n",
    "\r\n",
    "    if l1_rate is not None:\r\n",
    "        print(\"Training with L1 regularization\")\r\n",
    "    \r\n",
    "    if alpha is not None:\r\n",
    "        print(\"Training with early stopping: GL\")\r\n",
    "\r\n",
    "    model.to(device)\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    E_opt = None\r\n",
    "\r\n",
    "    # epoch loop\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "        performance_meter = AverageMeter()\r\n",
    "\r\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, l1_rate)\r\n",
    "\r\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\r\n",
    "\r\n",
    "        if alpha is not None:\r\n",
    "            E_va, _ = test_model(model, testloader, loss_fn=loss_fn)\r\n",
    "            E_opt = E_va if E_opt is None else min(E_opt, E_va)\r\n",
    "            GL = 100 * (E_va/E_opt - 1)\r\n",
    "            if GL > alpha:\r\n",
    "                print(f\"GL = {GL} > {alpha} - Stop training\")\r\n",
    "                return loss_meter.sum, performance_meter.avg\r\n",
    "\r\n",
    "    return loss_meter.sum, performance_meter.avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "minibatch_size_train = 256\r\n",
    "minibatch_size_test = 512\r\n",
    "\r\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)\r\n",
    "\r\n",
    "learn_rate = 0.1\r\n",
    "num_epochs = 30\r\n",
    "\r\n",
    "model = MLP()\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)\r\n",
    "\r\n",
    "train_loss, train_acc = train_model(model, trainloader, loss_fn, optimizer, num_epochs, alpha=5, device=\"cuda:0\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cuda:0\n",
      "Training with early stopping: GL\n",
      "Epoch 1 completed. Loss - total: 82812.12795066833 - average: 1.3802021325111389; Performance: 0.50945\n",
      "TESTING - loss 34024.57218551636 - performance 0.8098833333333333\n",
      "Epoch 2 completed. Loss - total: 24887.656105041504 - average: 0.4147942684173584; Performance: 0.8734166666666666\n",
      "TESTING - loss 23664.80724620819 - performance 0.87845\n",
      "Epoch 3 completed. Loss - total: 18715.289516448975 - average: 0.31192149194081625; Performance: 0.9067833333333334\n",
      "TESTING - loss 25743.16288471222 - performance 0.8646\n",
      "GL = 8.782474401252705 > 5 - Stop training\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. b."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another stopping criteria presented in the paper is based on the fact that if the training is still progressing rapidly, we might want to avoid stopping the algorithm. The reason is that if the training error is still decreasing quickly, there is more chance that the generalization losses can be somehow \"repaired\" and overfitting has probably not yet begun. The formalization of this reasoning is the definition of a so called _training strip of length k_, which is a training sequence of _k_ epochs ($n+1 ... n+k$, where $n$ is divisible by $k$). We can then measure the training progress (in per thousand) after a training strip as: $$P_k(t):=1000\\cdot\\left(\\dfrac{\\sum_{t'=t-k+1}^t E_{tr}(t')}{k\\cdot\\min_{t'=t-k+1}^t E_{tr}(t')}-1\\right)$$ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "which is a comparison between the average training error during the strip and the minimum training error during the same strip.\r\n",
    "\r\n",
    "The criterion tells to stop after the first end-of-strip epoch $t$ with $$\\dfrac{GL(t)}{P_k(t)}>\\alpha$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, alpha=None, k=None, l1_rate=None, performance=accuracy, device=None): # note: I've added a generic performance to replace accuracy and an object where to store the trajectory and the device on which to run our training\r\n",
    "\r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "    print(f\"Training on {device}\")\r\n",
    "\r\n",
    "    if l1_rate is not None:\r\n",
    "        print(\"Training with L1 regularization\")\r\n",
    "    \r\n",
    "    if alpha is not None and k is None:\r\n",
    "        print(\"Training with early stopping: GL\")\r\n",
    "\r\n",
    "    if k is not None:\r\n",
    "        print(\"Training with early stopping: PL\")\r\n",
    "        j = 1\r\n",
    "        strip = []\r\n",
    "\r\n",
    "    model.to(device)\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    E_opt = None\r\n",
    "\r\n",
    "    # epoch loop\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "        performance_meter = AverageMeter()\r\n",
    "\r\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, l1_rate)\r\n",
    "\r\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\r\n",
    "\r\n",
    "        if alpha is not None or k is not None:\r\n",
    "            E_va, _ = test_model(model, testloader, loss_fn=loss_fn)\r\n",
    "            E_opt = E_va if E_opt is None else min(E_opt, E_va)\r\n",
    "            GL = 100 * (E_va/E_opt - 1)\r\n",
    "            if k is None and GL > alpha:\r\n",
    "                print(f\"GL = {GL} > {alpha} - Stop training\")\r\n",
    "                return loss_meter.sum, performance_meter.avg\r\n",
    "            \r\n",
    "            if k is not None:\r\n",
    "                strip.append(loss_meter.sum)\r\n",
    "                if j % k != 0:\r\n",
    "                    j += 1\r\n",
    "                else:\r\n",
    "                    print(GL, strip, sum(strip), min(strip))\r\n",
    "                    Pk = 1000 * (sum(strip)/(k*min(strip)) - 1)\r\n",
    "                    if GL/Pk > alpha:\r\n",
    "                        print(f\"GL/Pk = {GL/Pk} > {alpha} - Stop training\")\r\n",
    "                        return loss_meter.sum, performance_meter.avg\r\n",
    "                    else:\r\n",
    "                        print(f\"GL/Pk = {GL/Pk} < {alpha} - Continue training\")\r\n",
    "                        j = 1\r\n",
    "                        strip = []                       \r\n",
    "\r\n",
    "    return loss_meter.sum, performance_meter.avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "minibatch_size_train = 256\r\n",
    "minibatch_size_test = 512\r\n",
    "\r\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)\r\n",
    "\r\n",
    "learn_rate = 0.1\r\n",
    "num_epochs = 30\r\n",
    "\r\n",
    "model = MLP()\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)\r\n",
    "\r\n",
    "train_loss, train_acc = train_model(model, trainloader, loss_fn, optimizer, num_epochs, alpha=0.5, k=5, device=\"cuda:0\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cuda:0\n",
      "Training with early stopping: PL\n",
      "Epoch 1 completed. Loss - total: 96991.61274909973 - average: 1.6165268791516623; Performance: 0.45568333333333333\n",
      "TESTING - loss 39627.57803916931 - performance 0.7799833333333334\n",
      "Epoch 2 completed. Loss - total: 29560.72462081909 - average: 0.4926787436803182; Performance: 0.846\n",
      "TESTING - loss 28653.597908973694 - performance 0.8504166666666667\n",
      "Epoch 3 completed. Loss - total: 20760.78544807434 - average: 0.346013090801239; Performance: 0.89635\n",
      "TESTING - loss 21107.402194023132 - performance 0.89295\n",
      "Epoch 4 completed. Loss - total: 16382.692322731018 - average: 0.273044872045517; Performance: 0.9178666666666667\n",
      "TESTING - loss 16101.186066627502 - performance 0.9167833333333333\n",
      "Epoch 5 completed. Loss - total: 14067.8447971344 - average: 0.23446407995224; Performance: 0.9294833333333333\n",
      "TESTING - loss 16175.528463363647 - performance 0.91615\n",
      "0.4617200026663326 [96991.61274909973, 29560.72462081909, 20760.78544807434, 16382.692322731018, 14067.8447971344] 177763.65993785858 14067.8447971344\n",
      "GL/Pk = 0.00030232438642421546 < 0.5 - Continue training\n",
      "Epoch 6 completed. Loss - total: 12520.651491641998 - average: 0.20867752486069996; Performance: 0.93695\n",
      "TESTING - loss 11937.348460197449 - performance 0.9402166666666667\n",
      "Epoch 7 completed. Loss - total: 11563.313785552979 - average: 0.19272189642588297; Performance: 0.9422\n",
      "TESTING - loss 11768.502126693726 - performance 0.9388666666666666\n",
      "Epoch 8 completed. Loss - total: 10841.300100803375 - average: 0.1806883350133896; Performance: 0.9450166666666666\n",
      "TESTING - loss 11051.252688407898 - performance 0.9444833333333333\n",
      "Epoch 9 completed. Loss - total: 10098.747701406479 - average: 0.168312461690108; Performance: 0.9487166666666667\n",
      "TESTING - loss 9913.907413005829 - performance 0.94955\n",
      "Epoch 10 completed. Loss - total: 9559.974842071533 - average: 0.15933291403452554; Performance: 0.9522833333333334\n",
      "TESTING - loss 16187.403510093689 - performance 0.9152333333333333\n",
      "63.27975273258861 [12520.651491641998, 11563.313785552979, 10841.300100803375, 10098.747701406479, 9559.974842071533] 54583.987921476364 9559.974842071533\n",
      "GL/Pk = 0.44585989408209525 < 0.5 - Continue training\n",
      "Epoch 11 completed. Loss - total: 9196.160121798515 - average: 0.15326933536330858; Performance: 0.9538166666666666\n",
      "TESTING - loss 9198.713651180267 - performance 0.9527166666666667\n",
      "Epoch 12 completed. Loss - total: 8759.70340847969 - average: 0.14599505680799485; Performance: 0.9564\n",
      "TESTING - loss 9505.69630765915 - performance 0.95075\n",
      "Epoch 13 completed. Loss - total: 8501.400499343872 - average: 0.14169000832239786; Performance: 0.9577\n",
      "TESTING - loss 19562.513060569763 - performance 0.8958666666666667\n",
      "Epoch 14 completed. Loss - total: 9303.138171434402 - average: 0.15505230285724003; Performance: 0.9534833333333333\n",
      "TESTING - loss 8598.60889673233 - performance 0.95605\n",
      "Epoch 15 completed. Loss - total: 7891.669468402863 - average: 0.13152782447338104; Performance: 0.9602\n",
      "TESTING - loss 8539.167392253876 - performance 0.9561666666666667\n",
      "0.0 [9196.160121798515, 8759.70340847969, 8501.400499343872, 9303.138171434402, 7891.669468402863] 43652.07166945934 7891.669468402863\n",
      "GL/Pk = 0.0 < 0.5 - Continue training\n",
      "Epoch 16 completed. Loss - total: 7742.756325006485 - average: 0.12904593875010809; Performance: 0.9610666666666666\n",
      "TESTING - loss 9785.897387504578 - performance 0.9494\n",
      "Epoch 17 completed. Loss - total: 7445.78231215477 - average: 0.12409637186924616; Performance: 0.9626333333333333\n",
      "TESTING - loss 11079.986754894257 - performance 0.9390166666666667\n",
      "Epoch 18 completed. Loss - total: 7344.744261264801 - average: 0.12241240435441335; Performance: 0.9631166666666666\n",
      "TESTING - loss 11401.331750392914 - performance 0.9421833333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('torchenv': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "cbf44f2bcbc60d09e119be1d56c4be5c34087e57cda6ccaa990ae958977726fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}