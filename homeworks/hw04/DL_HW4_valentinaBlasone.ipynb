{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 04 - Valentina Blasone\r\n",
    "\r\n",
    "## Deep Learning - A.A. 2020/2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!).\r\n",
    "The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works.\r\n",
    "> 2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530).\r\n",
    "*Tip*: To permute the labels, act on the `trainset.targets` with an appropriate torch function.\r\n",
    "Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so: `trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`. You can now use this `DataLoader` inside the training function.\r\n",
    "Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import os\r\n",
    "from torch import nn\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "\r\n",
    "from scripts import mnist\r\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class MLP(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.layers = nn.Sequential(\r\n",
    "            nn.Flatten(),\r\n",
    "            nn.Linear(28*28, 256),\r\n",
    "            nn.ReLU(),\r\n",
    "\r\n",
    "            nn.BatchNorm1d(num_features=256),\r\n",
    "            nn.Linear(256, 32),\r\n",
    "            nn.ReLU(),\r\n",
    "\r\n",
    "            nn.BatchNorm1d(num_features=32),\r\n",
    "            nn.Linear(32, 10)\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, X):\r\n",
    "        return self.layers(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device): # note: I've added a generic performance to replace accuracy and the device\r\n",
    "    for X, y in dataloader:\r\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\r\n",
    "        X = X.to(device)\r\n",
    "        y = y.to(device)\r\n",
    "        # ... like last time\r\n",
    "        optimizer.zero_grad() \r\n",
    "        y_hat = model(X)\r\n",
    "        loss = loss_fn(y_hat, y)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        acc = performance(y_hat, y)\r\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\r\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, epoch_start_scheduler=1, device=None):\r\n",
    "    # added lr_scheduler\r\n",
    "\r\n",
    "    # create the folder for the checkpoints (if it's not None)\r\n",
    "    if checkpoint_loc is not None:\r\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\r\n",
    "    \r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "    print(f\"Training on {device}\")   \r\n",
    "    \r\n",
    "    model.to(device)\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # epoch loop\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "        performance_meter = AverageMeter()\r\n",
    "\r\n",
    "        # added print for LR\r\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\r\n",
    "\r\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device=device)\r\n",
    "\r\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\r\n",
    "\r\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\r\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\r\n",
    "            checkpoint_dict = {\r\n",
    "                \"parameters\": model.state_dict(),\r\n",
    "                \"optimizer\": optimizer.state_dict(),\r\n",
    "                \"epoch\": epoch\r\n",
    "            }\r\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\r\n",
    "        \r\n",
    "        if lr_scheduler is not None:\r\n",
    "            if epoch >= epoch_start_scheduler:\r\n",
    "                lr_scheduler.step()\r\n",
    "            # or you can use a MultiStepLR with milestones=[6, 11] thus deleting the `if` construct for the epoch   \r\n",
    "\r\n",
    "    return loss_meter.sum, performance_meter.avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\r\n",
    "    # establish device\r\n",
    "    if device is None:\r\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "\r\n",
    "    # create an AverageMeter for the loss if passed\r\n",
    "    if loss_fn is not None:\r\n",
    "        loss_meter = AverageMeter()\r\n",
    "    \r\n",
    "    performance_meter = AverageMeter()\r\n",
    "\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        for X, y in dataloader:\r\n",
    "            X = X.to(device)\r\n",
    "            y = y.to(device)\r\n",
    "\r\n",
    "            y_hat = model(X)\r\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\r\n",
    "            acc = performance(y_hat, y)\r\n",
    "            if loss_fn is not None:\r\n",
    "                loss_meter.update(loss.item(), X.shape[0])\r\n",
    "            performance_meter.update(acc, X.shape[0])\r\n",
    "    # get final performances\r\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\r\n",
    "    fin_perf = performance_meter.avg\r\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\r\n",
    "    return fin_loss, fin_perf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "minibatch_size_train = 256\r\n",
    "minibatch_size_test = 512\r\n",
    "\r\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)\r\n",
    "\r\n",
    "learn_rate = 0.1\r\n",
    "num_epochs = 30\r\n",
    "\r\n",
    "model = MLP()\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9)\r\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_loss, train_acc = train_model(model, trainloader, loss_fn, optimizer, num_epochs, device=\"cuda:0\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cuda:0\n",
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 15269.52510690689 - average: 0.25449208511511484; Performance: 0.92635\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 4758.294134140015 - average: 0.07930490223566691; Performance: 0.9756333333333334\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 3037.1740114688873 - average: 0.05061956685781479; Performance: 0.9846166666666667\n",
      "Epoch 4 --- learning rate 0.10000\n",
      "Epoch 4 completed. Loss - total: 2128.6691996455193 - average: 0.03547781999409199; Performance: 0.9891666666666666\n",
      "Epoch 5 --- learning rate 0.10000\n",
      "Epoch 5 completed. Loss - total: 1453.3751927614212 - average: 0.02422291987935702; Performance: 0.99265\n",
      "Epoch 6 --- learning rate 0.10000\n",
      "Epoch 6 completed. Loss - total: 1143.6829409003258 - average: 0.019061382348338762; Performance: 0.99425\n",
      "Epoch 7 --- learning rate 0.10000\n",
      "Epoch 7 completed. Loss - total: 815.8666926026344 - average: 0.013597778210043907; Performance: 0.9960333333333333\n",
      "Epoch 8 --- learning rate 0.10000\n",
      "Epoch 8 completed. Loss - total: 555.750966489315 - average: 0.00926251610815525; Performance: 0.9975\n",
      "Epoch 9 --- learning rate 0.10000\n",
      "Epoch 9 completed. Loss - total: 365.15875494480133 - average: 0.006085979249080022; Performance: 0.9986166666666667\n",
      "Epoch 10 --- learning rate 0.10000\n",
      "Epoch 10 completed. Loss - total: 517.971855327487 - average: 0.008632864255458117; Performance: 0.9973333333333333\n",
      "Epoch 11 --- learning rate 0.10000\n",
      "Epoch 11 completed. Loss - total: 250.16849252581596 - average: 0.004169474875430266; Performance: 0.9991666666666666\n",
      "Epoch 12 --- learning rate 0.10000\n",
      "Epoch 12 completed. Loss - total: 212.917492300272 - average: 0.0035486248716712; Performance: 0.9992166666666666\n",
      "Epoch 13 --- learning rate 0.10000\n",
      "Epoch 13 completed. Loss - total: 150.9828281691298 - average: 0.0025163804694854964; Performance: 0.9995666666666667\n",
      "Epoch 14 --- learning rate 0.10000\n",
      "Epoch 14 completed. Loss - total: 89.73041820526123 - average: 0.0014955069700876872; Performance: 0.9997333333333334\n",
      "Epoch 15 --- learning rate 0.10000\n",
      "Epoch 15 completed. Loss - total: 139.57278286665678 - average: 0.002326213047777613; Performance: 0.9995833333333334\n",
      "Epoch 16 --- learning rate 0.10000\n",
      "Epoch 16 completed. Loss - total: 88.31129421293736 - average: 0.001471854903548956; Performance: 0.99975\n",
      "Epoch 17 --- learning rate 0.10000\n",
      "Epoch 17 completed. Loss - total: 63.523509338498116 - average: 0.0010587251556416352; Performance: 0.9999166666666667\n",
      "Epoch 18 --- learning rate 0.10000\n",
      "Epoch 18 completed. Loss - total: 42.88466411828995 - average: 0.0007147444019714992; Performance: 0.99995\n",
      "Epoch 19 --- learning rate 0.10000\n",
      "Epoch 19 completed. Loss - total: 27.008317349711433 - average: 0.00045013862249519053; Performance: 0.9999833333333333\n",
      "Epoch 20 --- learning rate 0.10000\n",
      "Epoch 20 completed. Loss - total: 22.715045124292374 - average: 0.0003785840854048729; Performance: 0.9999833333333333\n",
      "Epoch 21 --- learning rate 0.10000\n",
      "Epoch 21 completed. Loss - total: 19.04284963477403 - average: 0.00031738082724623384; Performance: 0.9999833333333333\n",
      "Epoch 22 --- learning rate 0.10000\n",
      "Epoch 22 completed. Loss - total: 16.5828760266304 - average: 0.0002763812671105067; Performance: 0.9999833333333333\n",
      "Epoch 23 --- learning rate 0.10000\n",
      "Epoch 23 completed. Loss - total: 14.591372542083263 - average: 0.00024318954236805438; Performance: 0.9999833333333333\n",
      "Epoch 24 --- learning rate 0.10000\n",
      "Epoch 24 completed. Loss - total: 13.563010333105922 - average: 0.00022605017221843203; Performance: 0.9999833333333333\n",
      "Epoch 25 --- learning rate 0.10000\n",
      "Epoch 25 completed. Loss - total: 12.606366084422916 - average: 0.0002101061014070486; Performance: 1.0\n",
      "Epoch 26 --- learning rate 0.10000\n",
      "Epoch 26 completed. Loss - total: 12.076888200826943 - average: 0.00020128147001378238; Performance: 1.0\n",
      "Epoch 27 --- learning rate 0.10000\n",
      "Epoch 27 completed. Loss - total: 9.643791218288243 - average: 0.0001607298536381374; Performance: 1.0\n",
      "Epoch 28 --- learning rate 0.10000\n",
      "Epoch 28 completed. Loss - total: 9.902229369152337 - average: 0.00016503715615253896; Performance: 1.0\n",
      "Epoch 29 --- learning rate 0.10000\n",
      "Epoch 29 completed. Loss - total: 9.194437626749277 - average: 0.00015324062711248796; Performance: 1.0\n",
      "Epoch 30 --- learning rate 0.10000\n",
      "Epoch 30 completed. Loss - total: 10.200727451592684 - average: 0.0001700121241932114; Performance: 0.9999833333333333\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "final_loss, final_perf = test_model(model, testloader, loss_fn=loss_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 9.037457719445229 - performance 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2\r\n",
    "\r\n",
    "Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530).\r\n",
    "*Tip*: To permute the labels, act on the `trainset.targets` with an appropriate torch function.\r\n",
    "Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so: `trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`. You can now use this `DataLoader` inside the training function.\r\n",
    "Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "trainset_permuted = trainset\r\n",
    "trainset_permuted.targets = trainset_permuted.targets[torch.randperm(trainset_permuted.targets.size()[0])]\r\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=minibatch_size_train, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "learn_rate = 0.1\r\n",
    "num_epochs = 30\r\n",
    "\r\n",
    "model = MLP()\r\n",
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=0.9)\r\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_loss, train_acc = train_model(model, trainloader_permuted, loss_fn, optimizer, num_epochs, device=\"cuda:0\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cuda:0\n",
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 139515.97901916504 - average: 2.325266316986084; Performance: 0.1023\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 138454.85108184814 - average: 2.307580851364136; Performance: 0.10558333333333333\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 138118.67540740967 - average: 2.301977923456828; Performance: 0.11113333333333333\n",
      "Epoch 4 --- learning rate 0.10000\n",
      "Epoch 4 completed. Loss - total: 137895.93157196045 - average: 2.2982655261993408; Performance: 0.1165\n",
      "Epoch 5 --- learning rate 0.10000\n",
      "Epoch 5 completed. Loss - total: 137695.47243499756 - average: 2.2949245405832928; Performance: 0.12113333333333333\n",
      "Epoch 6 --- learning rate 0.10000\n",
      "Epoch 6 completed. Loss - total: 137525.7042236328 - average: 2.2920950703938803; Performance: 0.12551666666666667\n",
      "Epoch 7 --- learning rate 0.10000\n",
      "Epoch 7 completed. Loss - total: 137271.4722290039 - average: 2.2878578704833985; Performance: 0.1272\n",
      "Epoch 8 --- learning rate 0.10000\n",
      "Epoch 8 completed. Loss - total: 137005.37174224854 - average: 2.283422862370809; Performance: 0.13416666666666666\n",
      "Epoch 9 --- learning rate 0.10000\n",
      "Epoch 9 completed. Loss - total: 136775.6733932495 - average: 2.2795945565541587; Performance: 0.13783333333333334\n",
      "Epoch 10 --- learning rate 0.10000\n",
      "Epoch 10 completed. Loss - total: 136386.15576934814 - average: 2.2731025961558022; Performance: 0.1421\n",
      "Epoch 11 --- learning rate 0.10000\n",
      "Epoch 11 completed. Loss - total: 136026.70561218262 - average: 2.2671117602030435; Performance: 0.14733333333333334\n",
      "Epoch 12 --- learning rate 0.10000\n",
      "Epoch 12 completed. Loss - total: 135684.6742324829 - average: 2.2614112372080486; Performance: 0.1514\n",
      "Epoch 13 --- learning rate 0.10000\n",
      "Epoch 13 completed. Loss - total: 135391.40091705322 - average: 2.2565233486175535; Performance: 0.15518333333333334\n",
      "Epoch 14 --- learning rate 0.10000\n",
      "Epoch 14 completed. Loss - total: 134864.23845672607 - average: 2.2477373076121014; Performance: 0.15966666666666668\n",
      "Epoch 15 --- learning rate 0.10000\n",
      "Epoch 15 completed. Loss - total: 134435.9743118286 - average: 2.24059957186381; Performance: 0.16426666666666667\n",
      "Epoch 16 --- learning rate 0.10000\n",
      "Epoch 16 completed. Loss - total: 133871.87371063232 - average: 2.2311978951772056; Performance: 0.17196666666666666\n",
      "Epoch 17 --- learning rate 0.10000\n",
      "Epoch 17 completed. Loss - total: 133325.3483352661 - average: 2.2220891389211017; Performance: 0.17733333333333334\n",
      "Epoch 18 --- learning rate 0.10000\n",
      "Epoch 18 completed. Loss - total: 132706.82986450195 - average: 2.211780497741699; Performance: 0.1817\n",
      "Epoch 19 --- learning rate 0.10000\n",
      "Epoch 19 completed. Loss - total: 131998.14379882812 - average: 2.199969063313802; Performance: 0.18835\n",
      "Epoch 20 --- learning rate 0.10000\n",
      "Epoch 20 completed. Loss - total: 131470.43071746826 - average: 2.1911738452911376; Performance: 0.19125\n",
      "Epoch 21 --- learning rate 0.10000\n",
      "Epoch 21 completed. Loss - total: 130905.43099212646 - average: 2.181757183202108; Performance: 0.19796666666666668\n",
      "Epoch 22 --- learning rate 0.10000\n",
      "Epoch 22 completed. Loss - total: 130125.6743850708 - average: 2.16876123975118; Performance: 0.20515\n",
      "Epoch 23 --- learning rate 0.10000\n",
      "Epoch 23 completed. Loss - total: 129199.01200866699 - average: 2.1533168668111164; Performance: 0.21046666666666666\n",
      "Epoch 24 --- learning rate 0.10000\n",
      "Epoch 24 completed. Loss - total: 128476.47336578369 - average: 2.141274556096395; Performance: 0.2184\n",
      "Epoch 25 --- learning rate 0.10000\n",
      "Epoch 25 completed. Loss - total: 128046.84267425537 - average: 2.134114044570923; Performance: 0.22078333333333333\n",
      "Epoch 26 --- learning rate 0.10000\n",
      "Epoch 26 completed. Loss - total: 127055.27309417725 - average: 2.117587884902954; Performance: 0.2287\n",
      "Epoch 27 --- learning rate 0.10000\n",
      "Epoch 27 completed. Loss - total: 126207.4842376709 - average: 2.1034580706278483; Performance: 0.23428333333333334\n",
      "Epoch 28 --- learning rate 0.10000\n",
      "Epoch 28 completed. Loss - total: 125855.20642852783 - average: 2.097586773808797; Performance: 0.23868333333333333\n",
      "Epoch 29 --- learning rate 0.10000\n",
      "Epoch 29 completed. Loss - total: 124903.18047332764 - average: 2.0817196745554605; Performance: 0.24451666666666666\n",
      "Epoch 30 --- learning rate 0.10000\n",
      "Epoch 30 completed. Loss - total: 124027.88472747803 - average: 2.067131412124634; Performance: 0.2518166666666667\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "final_loss, final_perf = test_model(model, testloader, loss_fn=loss_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 120631.57796859741 - performance 0.2811\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('torchenv': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "cbf44f2bcbc60d09e119be1d56c4be5c34087e57cda6ccaa990ae958977726fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}