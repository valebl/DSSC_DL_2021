{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd07bd19c094dc44574d64e00d65102e6ba36e1076719a93266f721ddb81f822a22",
   "display_name": "Python 3.7.3 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 7\n",
    "\n",
    "## ConvNets 101"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* we introduced some basic image processing functionalities with OpenCV\n",
    "* we saw how to import a custom dataset in PyTorch, how to operate data augmentation, and how to create a DataLoader out of it\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* we will construct our first Convolutional Neural Network (CNN)\n",
    "* we will show how to do transfer learning on CNNs\n",
    "* we will show how to introduce Deconvolution/Inverse Convolution inside CNNs\n",
    "* we will construct our own implementation of the ResNet\n",
    "* we will learn about the U-Net, a CNN for image segmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Our first CNN\n",
    "\n",
    "Building CNNs is actually not hard once you know all the pieces to construct a MultiLayer Perceptron.\n",
    "\n",
    "We can distinguish between two macro-categories of CNNs, at least for the part concerning image classification. We might call them \"historical\" and \"modern\", although characteristics of both can sometimes get pretty much mixed-up.\n",
    "\n",
    "* \"Classical\" (\"Historical\") CNNs are a stack composed of two parts:\n",
    "  * a **convolutional** part, where have a cascade of convolutional layers intertwined with pooling layers for dimensionality and complexity reduction\n",
    "    * usually the filters in each convolutional layer are more numerous as the image size shrinks (i.e., as we get further from the input)\n",
    "  * a **fully-connected** part, where we have a sequence of few fully-connected layer, ending up in the output layer, where, as usual, we have as many neurons as there are categories\n",
    "  \n",
    " the epitome of the historical CNN (which is still used in research today nonetheless) is VGGNet (or simply VGG) [5](https://arxiv.org/abs/1409.1556v6). Its *core* is a **convolutional block** composed of two or three convolutional layers each with the same number of filters, which is double the number of filters of the previous layer, up to 512 filters per layer. At the end of each block, there's a Max Pooling layer which halves the spatial dimension of the image.\n",
    "\n",
    " In the picture below, you can see a modern implementation of VGG with only one fully-connected layer.\n",
    "  ![](img/vgg.png)\n",
    "  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* \"Modern\" CNNs, instead, get rid of the fully-connected part, as usually\n",
    "    1. it introduces a considerable amount of parameters in the network (note that the convolutional layers, due to their local connectivity and shared weights, have a much lower number of parameters w.r.t. fully connected layers)\n",
    "    2. it keeps the 2D spatial structure of the image intact up to the last hidden layer, allowing for more interpretal parameters/neurons (*insert Olah citation here*)\n",
    "    3. it represents a \"rigid\" portion of the network in the sense that it constrains they size of the image to be fixed. We will see later how this can pose a problem.\n",
    "\n",
    " Sometimes, even the pooling layers may get replaced by convolutional layers with large kernel size, as its effect is to reduce dimensionality (i.e., height and width) of the corresponding image.\n",
    " \n",
    " Recently, the **residual block** has become one of the paramount structures in modern CNNs.\n",
    "\n",
    " \"Classical\" CNNs stack a large number of convolutional layers on top of each other. This though can lead to problems such as the **vanishing gradient**. In a periodic fashion, resnets introduce **skip connections**:\n",
    "    * given the layer of index $k$, we call $a_k$ its output/activation\n",
    "    * $a_k$ gets pushed towards the next layer $k+1$, but we also \"propagate\" it *after* the layer $k+m$ (usually $m$ is 1 or 2) and we sum $a_k$ to $a_{k+m}$.\n",
    "    \n",
    " More on that later.\n",
    "\n",
    " ![](img/resnet.png)\n",
    "\n",
    " In the diagram above there're two residual blocks. The skip connections are indicated with the protruding connector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg11_bn\n",
    "from torchsummary import summary\n",
    "from scripts import mnistm\n",
    "from scripts import mnist\n",
    "from scripts import train\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def _build_vgg_block(self, num_conv_layers, in_channels, out_channels, kernel_size=3, batchnorm=True, activation=nn.ReLU, maxpool=True):\n",
    "        layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            if i == 0:\n",
    "                num_channels_in = in_channels\n",
    "            else:\n",
    "                num_channels_in = out_channels\n",
    "            \n",
    "            layers.append(nn.Conv2d(in_channels=num_channels_in, out_channels=out_channels, kernel_size=kernel_size))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            self._build_vgg_block(2, in_channels, 16, activation=nn.SiLU),\n",
    "            self._build_vgg_block(2, 16, 32, activation=nn.SiLU)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classifier(self.avgpool(self.features(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Sequential: 1-1                        --\n|    └─Sequential: 2-1                   --\n|    |    └─Conv2d: 3-1                  448\n|    |    └─BatchNorm2d: 3-2             32\n|    |    └─SiLU: 3-3                    --\n|    |    └─Conv2d: 3-4                  2,320\n|    |    └─BatchNorm2d: 3-5             32\n|    |    └─SiLU: 3-6                    --\n|    |    └─MaxPool2d: 3-7               --\n|    └─Sequential: 2-2                   --\n|    |    └─Conv2d: 3-8                  4,640\n|    |    └─BatchNorm2d: 3-9             64\n|    |    └─SiLU: 3-10                   --\n|    |    └─Conv2d: 3-11                 9,248\n|    |    └─BatchNorm2d: 3-12            64\n|    |    └─SiLU: 3-13                   --\n|    |    └─MaxPool2d: 3-14              --\n├─AdaptiveAvgPool2d: 1-2                 --\n├─Sequential: 1-3                        --\n|    └─Flatten: 2-3                      --\n|    └─Linear: 2-4                       330\n=================================================================\nTotal params: 17,178\nTrainable params: 17,178\nNon-trainable params: 0\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "net = CNN(in_channels=3)\n",
    "_ = summary(net)"
   ]
  },
  {
   "source": [
    "Test if the net works on random data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X = torch.rand((100,3,28,28))\n",
    "y = net(X)\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "Let's go more in detail in the computation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in torch.Size([100, 3, 28, 28])\n0 torch.Size([100, 16, 12, 12])\n1 torch.Size([100, 32, 4, 4])\navgpool torch.Size([100, 32, 1, 1])\nout torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "z = X\n",
    "print(\"in\", z.shape)\n",
    "for i, mod in enumerate(net.features):\n",
    "    z = mod(z)\n",
    "    print(i, z.shape)\n",
    "\n",
    "z = net.avgpool(z)\n",
    "print(\"avgpool\", z.shape)\n",
    "\n",
    "z = net.classifier(z)\n",
    "print(\"out\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz\n",
      "Processing...\n",
      "C:\\Users\\mzullich\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Users\\mzullich\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4639, 0.4676, 0.4199], [0.2534, 0.2380, 0.2618]), # I pre-computed these data\n",
    "])\n",
    "\n",
    "mnistm_train = mnistm.MNISTM(root=\"datasets/MNISTM\", download=True, transform=transforms)\n",
    "mnistm_test = mnistm.MNISTM(root=\"datasets/MNISTM\", train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ground truth: 5\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p7d5f6b1543)\">\r\n    <image height=\"218\" id=\"imagedc3bafd9e5\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0c2bff9374\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m0c2bff9374\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma60f45dbdb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma60f45dbdb\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p7d5f6b1543\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "img = (mnistm_train.train_data[0].numpy() * 255).astype(\"uint8\")\n",
    "plt.imshow(img)\n",
    "print(\"Ground truth:\", mnistm_train.train_labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28, 3]), torch.Size([10000]))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "mnistm_test.test_data.shape, mnistm_test.test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = 128\n",
    "batch_test = 512\n",
    "mnistm_trainloader = DataLoader(mnistm_train, batch_train, shuffle=True, num_workers=8)\n",
    "mnistm_testloader = DataLoader(mnistm_test, batch_test, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=5e-4, momentum=.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-de01cbbe4fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnistm_trainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\dl-course\\labs\\scripts\\train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc, checkpoint_name, performance, lr_scheduler, device, lr_scheduler_step_on_epoch)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mlr_scheduler_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlr_scheduler_step_on_epoch\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}; Performance: {performance_meter.avg:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dl-course\\labs\\scripts\\train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, lr_scheduler)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# note: I've added a generic performance to replace accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    866\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.train_model(net, mnistm_trainloader, loss_fn=nn.CrossEntropyLoss(), optimizer=optim, num_epochs=50, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models_push/cnn_mnistm/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"models_push/cnn_mnistm/model.pt\")) #-> Now model is on CPU!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.9790\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.979)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "train.test_model(net, mnistm_testloader)"
   ]
  },
  {
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Transfer Learning is the utilization of a model, built for a given task (**upstream task**) on one or more different tasks (**downstream task**).\n",
    "\n",
    "Usually, in Deep Learning we operate a **model finetuning**, i.e., we use a model fully-trained on the upstream task and we adapt it someway on the downstream task.\n",
    "\n",
    "Fine-tuning comes in different flavors:\n",
    "\n",
    "1. we might just re-use the model for the upstream task without re-training\n",
    "\n",
    "2. we might want to re-train the model on the downstream task leaving fixed the parameters of the majority of the layers, while re-training only the remaining parameters.\n",
    " Usually, we train only the weights in the final fully-connected layer. This method is called **feature extraction**.\n",
    "\n",
    "3. we might want to fully re-train the model, using the `state_dict` from the upstream task as an **initialization** for the training on the downstream task. This is the proper **fine-tuning**.\n",
    "\n",
    "NB: if the ground truth for the downstream task and the upstream task have different domain (e.g. different number of categories), I'll need to modify the architecture of my CNN by **changing the number of output features** of the last layer. This, of course, will mean that the weights in this layer need to be re-initialized, hence, we'll for sure need to resort to strategies 2 or 3.\n",
    "\n",
    "NB2: a common techinque which is operated for **small downstream tasks datasets** is to obtain the features before classification (or, in principle, before any one of the layers of my CNN) and to operate feature extraction by fitting a **linear SVM** on top of these features (similar, in principle, to re-training on the last layer only, but with different loss). This approach has been shown to be much more robust to small dataset size.\n",
    "\n",
    "#### Re-use the same model on MNIST\n",
    "\n",
    "Basically, we wish to do this:\n",
    "\n",
    "![](img/tl1.jpg)\n",
    "\n",
    "We just need to import MNIST."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "_, _, mnist_train, mnist_test = mnist.get_data()\n",
    "mnist_train.data.shape"
   ]
  },
  {
   "source": [
    "Have a look at the shape.\n",
    "\n",
    "**Q**: What do we need to do?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_3(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        n, h, w = mnist_dataset.data.shape\n",
    "        self.data = mnist_dataset.data.clone().unsqueeze(1).expand(n, 3, h, w)\n",
    "        self.targets = mnist_dataset.targets.clone()\n",
    "        if transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.data[index]), self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_mnist = T.Compose([\n",
    "    lambda x: x/255,\n",
    "    T.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081])\n",
    "])\n",
    "mnist3_train = MNIST_3(mnist_train, transforms_mnist)\n",
    "mnist3_test = MNIST_3(mnist_test, transforms_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20c9dd73518>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p6b45ca9cbe)\">\r\n    <image height=\"218\" id=\"imagec115e80139\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf92539f244\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mf92539f244\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m36f69a07b5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m36f69a07b5\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6b45ca9cbe\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(mnist3_train.data[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainloader = DataLoader(mnist3_train, 512, shuffle=False, num_workers=0)\n",
    "mnist_testloader = DataLoader(mnist3_test, 512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.8296\n",
      "TESTING - loss -- - performance 0.8285\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.8285)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "train.test_model(net, mnist_trainloader)\n",
    "train.test_model(net, mnist_testloader)"
   ]
  },
  {
   "source": [
    "We see that the performance is *kinda good*, but we'd expect more out of a model on MNIST.\n",
    "\n",
    "#### Feature extraction\n",
    "\n",
    "Basically we do this:\n",
    "\n",
    "![](img/tl2.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "What we need to do:\n",
    "\n",
    "1. Freeze the training in the layers from the first to the second-to-last by *inhibiting* the `grad` in these layers\n",
    "2. Create the optimizer passing only the parameters of the last layer\n",
    "\n",
    "Notice that, if we consider the `named_parameters` of our net, the weight and bias of the last layer share the name `classifier.1`. Knowing that, we can inhibit the gradient by setting `param.requires_grad` to `False`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pars in net.named_parameters():\n",
    "    if \"classifier.1\" not in name:\n",
    "        pars.requires_grad = False"
   ]
  },
  {
   "source": [
    "Mirroring the strategy above, we can *filter* the parameters being trained by the optimizer with a list comprehension:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_finetune = torch.optim.SGD([pars for name, pars in net.named_parameters() if \"classifier.1\" in name], lr=.1, momentum=.9, weight_decay=5e-4)\n",
    "train.train_model(net, mnist_trainloader, nn.CrossEntropyLoss(), optim_finetune, 5)"
   ]
  },
  {
   "source": [
    "### Proper finetuning (ILSVRC2012 → CIFAR10)\n",
    "\n",
    "`torchvision` enables us to directly download pre-trained models from the web. Normally, we can access the weights of a large array of CNNs pretrained on the large-scale image classification dataset `ILSVRC2012`, also known as ImageNet, although the proper ImageNet dataset is a larger version of `ILSVRC2012`.\n",
    "\n",
    "These CNNs have the last layer whose output size is 1000 (the number of classes in `ILSVRC2012`).\n",
    "\n",
    "We wish to fine-tune a pre-trained `vgg11` model on the CIFAR10 dataset.\n",
    "\n",
    "This dataset is comparable in size to MNIST, with 10 categories:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/16641054/46775076-8b17e480-cd40-11e8-9501-89c6fbca36bd.jpg)\n",
    "\n",
    "The images are slightly larger ($32 \\times 32$ instead of $28 \\times 28$) and are encoded as RGB (→ 3 channels).\n",
    "\n",
    "Let's import the dataset and create the DataLoaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cifar = T.Compose([T.ToTensor(), T.Normalize([0.4913997551666284, 0.48215855929893703, 0.4465309133731618], [0.24703225141799082, 0.24348516474564, 0.26158783926049628])])\n",
    "\n",
    "cifar_train = CIFAR10(\"datasets/CIFAR\", train=True, transform=transform_cifar, download=True)\n",
    "cifar_test = CIFAR10(\"datasets/CIFAR\", train=False, transform=transform_cifar, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_trainloader = DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "cifar_testloader = DataLoader(cifar_test, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of trainset\", cifar_train.data.shape)\n",
    "print(\"Num classes\", max(cifar_test.targets) + 1)"
   ]
  },
  {
   "source": [
    "#### VGG11 \n",
    "\n",
    "VGG11 is a network similar to the CNN we created before:\n",
    "\n",
    "![](https://www.researchgate.net/publication/336550999/figure/fig1/AS:814110748987402@1571110536721/Figure-S1-Block-diagram-of-the-VGG11-architecture-Adapted-from-https-bitly-2ksX5Eq.png)\n",
    "\n",
    "In addition to the picture above, we also use batchnorm (at the time of the invention of VGG, batchnorm wasn't known)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg11_bn(pretrained=True)\n",
    "vgg.classifier[-1].out_features = 10 # update the num_classes to reflect the new dataset"
   ]
  },
  {
   "source": [
    "Let's see how the network performs without fine-tuning:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.test_model(vgg, cifar_testloader)"
   ]
  },
  {
   "source": [
    "In the next cells, you'll find how to fine-tune the CNN. It takes around 10 mins on single GPU, so I leave the commands and output as markdown notes.\n",
    "\n",
    "*You can find the weights under the `models_push/vgg11_cifar` folder.* Note: since the state_dicts are very heavy, I transfered them on Teams (folder `Additional Material for Labs`).\n",
    "\n",
    "Then, I'll also show how to train the `VGG11_bn` from scratch and leave the output of the training. You can clearly notice how the performance of the fine-tuned model is much better after 20 epochs (actually, this is not shown in the output, but the performance at epoch 1 is much higher in the pre-trained model).\n",
    "\n",
    "With proper data augmentation and lr scheduling (and maybe a deeper CNN), we can actually reach a test-set accuracy of 94% or higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "optimizer = torch.optim.Adam(vgg.parameters())\n",
    "train.train_model(vgg, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 5852.242087006569 - average: 0.11704484174013138; Performance: 0.96644\n",
    "\n",
    "Epoch 11 completed. Loss - total: 5551.681170940399 - average: 0.11103362341880799; Performance: 0.9682\n",
    "\n",
    "Epoch 12 completed. Loss - total: 5018.1823744773865 - average: 0.10036364748954774; Performance: 0.9718\n",
    "\n",
    "Epoch 13 completed. Loss - total: 17709.622314095497 - average: 0.35419244628190993; Performance: 0.92888\n",
    "\n",
    "Epoch 14 completed. Loss - total: 17280.205318450928 - average: 0.34560410636901856; Performance: 0.89974\n",
    "\n",
    "Epoch 15 completed. Loss - total: 3871.2677970528603 - average: 0.0774253559410572; Performance: 0.97692\n",
    "\n",
    "Epoch 16 completed. Loss - total: 2241.83546949178 - average: 0.0448367093898356; Performance: 0.9872\n",
    "\n",
    "Epoch 17 completed. Loss - total: 2088.5263759568334 - average: 0.041770527519136666; Performance: 0.9882\n",
    "\n",
    "Epoch 18 completed. Loss - total: 2305.237785771489 - average: 0.04610475571542978; Performance: 0.98758\n",
    "\n",
    "Epoch 19 completed. Loss - total: 2979.0519604235888 - average: 0.05958103920847178; Performance: 0.98364\n",
    "\n",
    "Epoch 20 completed. Loss - total: 3053.6747498363256 - average: 0.06107349499672651; Performance: 0.98368\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "train.test_model(vgg, cifar_testloader)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.8552\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "torch.save(vgg.state_dict, \"models/vgg11_cifar/vgg11_tl.pt\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's compare it with a VGG11_bn trained from scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "vgg2 = vgg11_bn(pretrained=False, num_classes=10)\n",
    "optimizer = torch.optim.Adam(vgg2.parameters())\n",
    "train.train_model(vgg2, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 48581.04235076904 - average: 0.9716208470153809; Performance: 0.66648\n",
    "\n",
    "Epoch 11 completed. Loss - total: 39186.03456878662 - average: 0.7837206913757324; Performance: 0.72996\n",
    "\n",
    "Epoch 12 completed. Loss - total: 35287.52504825592 - average: 0.7057505009651184; Performance: 0.76124\n",
    "\n",
    "Epoch 13 completed. Loss - total: 38614.65808200836 - average: 0.7722931616401673; Performance: 0.7412\n",
    "\n",
    "Epoch 14 completed. Loss - total: 30812.80309009552 - average: 0.6162560618019104; Performance: 0.79158\n",
    "\n",
    "Epoch 15 completed. Loss - total: 30947.483282089233 - average: 0.6189496656417847; Performance: 0.79536\n",
    "\n",
    "Epoch 16 completed. Loss - total: 30550.97905731201 - average: 0.6110195811462402; Performance: 0.80206\n",
    "\n",
    "Epoch 17 completed. Loss - total: 23598.16388607025 - average: 0.471963277721405; Performance: 0.85028\n",
    "\n",
    "Epoch 18 completed. Loss - total: 32089.43673324585 - average: 0.641788734664917; Performance: 0.80358\n",
    "\n",
    "Epoch 19 completed. Loss - total: 37266.2276263237 - average: 0.745324552526474; Performance: 0.78556\n",
    "\n",
    "Epoch 20 completed. Loss - total: 23250.735929965973 - average: 0.4650147185993195; Performance: 0.84466\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "train.test_model(vgg2, cifar_testloader)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.7927\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "torch.save(vgg.state_dict(), \"models/vgg11_cifar/vgg11_scratch.pt\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### ResNets [3](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)\n",
    "\n",
    "As told before, the building block of resnets is the **residual block**, which enables the network to train faster and avoid the vanishing gradient problem:\n",
    "\n",
    "![](https://www.aiuai.cn/uploads/sina/5ce8dfe46e373.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From the image above, we see two popular implementations of the residual block.\n",
    "\n",
    "* the first implementation is the simplest one. We have two $3\\times 3$, 1-padding, convolutions and the same number of filters as the incoming data.\n",
    "\n",
    " We can obtain the spatial dimension of the output (i.e., height and width) with this formula (found in [1](https://arxiv.org/abs/1603.07285)): $\\text{output_size} = \\text{in_size} + 2\\cdot \\text{pad} - \\text{ker_size} + 1$. Since $p=1,~\\text{pad}=2,~\\text{ker_size}=3$, then $\\text{output_size} = \\text{in_size}$. Since we do not vary the number of channels in the data, we can safely sum the incoming data to the output of the second convolutional layer.\n",
    "    * In some layers (architecture-dependent) a *subsampling* may be required (i.e., we need to decrease the spatial dimensions of the data). This is solved by applying a convolution with stride of 2 (the moving window *shifts* by 2 pixels instead of 1).\n",
    "     \n",
    "     In this case, **we need to downsample the data in the skip connections** as well. This is done via **$1\\times 1$ convolutions with stride of 2**.\n",
    "\n",
    " Let's implement this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, downsample=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, stride=(2 if downsample else 1))\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias)\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        if downsample:\n",
    "            # correction!! Padding is 0 otherwise dimensions don't match!!\n",
    "            self.conv_down = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, stride=2, padding=0, bias=bias)\n",
    "            self.bnorm_down = nn.BatchNorm2d(n_channels) # note that this is necessary in order to put data in the skip connection and data in the regular stream on the same domain!\n",
    "        \n",
    "        self.downsample = downsample\n",
    "            \n",
    "    def forward(self, X):\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        out = self.bnorm2(self.conv2(out))\n",
    "\n",
    "        #### SKIP CONNECTION\n",
    "        skip = X\n",
    "        if self.downsample:\n",
    "            skip = self.bnorm_down(self.conv_down(X))  \n",
    "        out = self.activ2(out + skip)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "* the second implementation is also called a **bottleneck layer** and operates an expansion of the number of channels (in a fashion similar to VGG). Differently from the residual block:\n",
    "    * the bottleneck has 3 instead of 2 convolutional layers\n",
    "        * the 1st and 3rd layer operate a $1\\times 1$ correlation with no padding (thus, leaving the image untouched in size)\n",
    "        * the 2nd is a regular $3\\times 3$ correlation which we saw before (padding of 1); however this can be a \"**dilated convolution**\":\n",
    "         ![](https://www.researchgate.net/publication/320195101/figure/fig2/AS:669211164692494@1536563783748/Dilated-convolution-On-the-left-we-have-the-dilated-convolution-with-dilation-rate-r.png)\n",
    "    * the width of the convolutional layers is controlled by a parameter (it should be $\\geq$ than the width of the incoming data -- canonical values are 64, 128...)\n",
    "    * the output of the block has a number of channels which is usually $\\times 4$ the width of the block.\n",
    "    * if a **downsampling** is required, it's operated by setting the stride to 2 for the second convolutional layer only.\n",
    "     \n",
    "     As in the basic Residual Block, the input is downsampled via a $1\\times 1$ convolution with stride 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, dilation_rate=1, expansion_factor=4, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, dilation=dilation_rate,  stride=(2 if downsample else 1))\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        out_channels = n_channels * expansion_factor\n",
    "        self.conv3 = nn.Conv2d(in_channels=n_channels, out_channels=out_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activ3 = activ()\n",
    "\n",
    "        if downsample:\n",
    "            self.down_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0, stride=2)\n",
    "            self.down_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, X):\n",
    "        # prints for clarity on data dimensionality\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        print(out.shape)\n",
    "        out = self.activ2(self.bnorm2(self.conv2(out)))\n",
    "        print(out.shape)\n",
    "        out = self.bnorm3(self.conv3(out))\n",
    "        print(out.shape)\n",
    "        \n",
    "        #### SKIP CONNECTION\n",
    "        skip = X\n",
    "        if self.downsample:\n",
    "            skip = self.down_bn(self.down_conv(X))\n",
    "        print(skip.shape)\n",
    "        out += skip\n",
    "        out = self.activ3(out)\n",
    "        print(\"---\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([24, 64, 100, 100])\n",
      "torch.Size([24, 64, 100, 100])\n",
      "torch.Size([24, 256, 100, 100])\n",
      "torch.Size([24, 256, 100, 100])\n",
      "---\n",
      "torch.Size([24, 64, 100, 100])\n",
      "torch.Size([24, 64, 100, 100])\n",
      "torch.Size([24, 256, 100, 100])\n",
      "torch.Size([24, 256, 100, 100])\n",
      "---\n",
      "torch.Size([24, 128, 100, 100])\n",
      "torch.Size([24, 128, 50, 50])\n",
      "torch.Size([24, 512, 50, 50])\n",
      "torch.Size([24, 512, 50, 50])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    BottleneckBlock(256, 64), # dimension will be 256→64→64→256\n",
    "    BottleneckBlock(256, 64),\n",
    "    BottleneckBlock(256, 128, downsample=True) # dimension will be 256→128→128→512 with downsampling of spatial dims\n",
    ")\n",
    "X = torch.rand((24, 256, 100, 100))\n",
    "_ = net(X)"
   ]
  },
  {
   "source": [
    "#### Resnet18\n",
    "\n",
    "Resnet18 is the simplest resnet architecture (18 total layers) presented in [2]. Let's see how we can implement it:\n",
    "\n",
    "![](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "From the image above, we can see that the Resnet18 is composed of one initial convolution (preparatory layer), four residual blocks, and one flattening/classification block."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### Preparatory layer\n",
    "\n",
    "Is a $7\\times 7$ convolution with stride 2 and 64 output channels, followed by batchnorm, relu, and maxpooling with a $3\\times 3$ kernel (!) and stride 2.\n",
    "\n",
    "Its function is mainly to reduce the image in size while extracting a large array of low-level features.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetPrep(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, activ=nn.ReLU, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=7, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activ(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "source": [
    "##### Residual blocks\n",
    "\n",
    "We have eight residual blocks with increasing number of channels, VGG-style (64,64→128,128→256,256→512,512).\n",
    "\n",
    "Whenever the number of channels is increased, we have also a subsampling.\n",
    "\n",
    "##### Average pooling and classification\n",
    "\n",
    "We flatten the array with \"adaptive average pooling\", which is a nice way to say that we're averaging across width.\n",
    "\n",
    "Now, we can start preparing our Resnet18 class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_channels=3, base_width=64, activ=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.prep = ResNetPrep(in_channels=in_channels, out_channels=base_width, activ=activ)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width * 2, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 2, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 4, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 4, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 8, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 8, n_channels=base_width * 8, activ=activ)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = nn.Linear(in_features=base_width*8, out_features=num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.prep(X)\n",
    "        out = self.res_blocks(out)\n",
    "        out = self.classifier(self.avgpool(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─ResNetPrep: 1-1                        --\n|    └─Sequential: 2-1                   --\n|    |    └─Conv2d: 3-1                  9,408\n|    |    └─BatchNorm2d: 3-2             128\n|    |    └─ReLU: 3-3                    --\n|    |    └─MaxPool2d: 3-4               --\n├─Sequential: 1-2                        --\n|    └─ResidualBlock: 2-2                --\n|    |    └─Conv2d: 3-5                  36,864\n|    |    └─BatchNorm2d: 3-6             128\n|    |    └─ReLU: 3-7                    --\n|    |    └─Conv2d: 3-8                  36,864\n|    |    └─BatchNorm2d: 3-9             128\n|    |    └─ReLU: 3-10                   --\n|    └─ResidualBlock: 2-3                --\n|    |    └─Conv2d: 3-11                 36,864\n|    |    └─BatchNorm2d: 3-12            128\n|    |    └─ReLU: 3-13                   --\n|    |    └─Conv2d: 3-14                 36,864\n|    |    └─BatchNorm2d: 3-15            128\n|    |    └─ReLU: 3-16                   --\n|    └─ResidualBlock: 2-4                --\n|    |    └─Conv2d: 3-17                 73,728\n|    |    └─BatchNorm2d: 3-18            256\n|    |    └─ReLU: 3-19                   --\n|    |    └─Conv2d: 3-20                 147,456\n|    |    └─BatchNorm2d: 3-21            256\n|    |    └─ReLU: 3-22                   --\n|    |    └─Conv2d: 3-23                 8,192\n|    |    └─BatchNorm2d: 3-24            256\n|    └─ResidualBlock: 2-5                --\n|    |    └─Conv2d: 3-25                 147,456\n|    |    └─BatchNorm2d: 3-26            256\n|    |    └─ReLU: 3-27                   --\n|    |    └─Conv2d: 3-28                 147,456\n|    |    └─BatchNorm2d: 3-29            256\n|    |    └─ReLU: 3-30                   --\n|    └─ResidualBlock: 2-6                --\n|    |    └─Conv2d: 3-31                 294,912\n|    |    └─BatchNorm2d: 3-32            512\n|    |    └─ReLU: 3-33                   --\n|    |    └─Conv2d: 3-34                 589,824\n|    |    └─BatchNorm2d: 3-35            512\n|    |    └─ReLU: 3-36                   --\n|    |    └─Conv2d: 3-37                 32,768\n|    |    └─BatchNorm2d: 3-38            512\n|    └─ResidualBlock: 2-7                --\n|    |    └─Conv2d: 3-39                 589,824\n|    |    └─BatchNorm2d: 3-40            512\n|    |    └─ReLU: 3-41                   --\n|    |    └─Conv2d: 3-42                 589,824\n|    |    └─BatchNorm2d: 3-43            512\n|    |    └─ReLU: 3-44                   --\n|    └─ResidualBlock: 2-8                --\n|    |    └─Conv2d: 3-45                 1,179,648\n|    |    └─BatchNorm2d: 3-46            1,024\n|    |    └─ReLU: 3-47                   --\n|    |    └─Conv2d: 3-48                 2,359,296\n|    |    └─BatchNorm2d: 3-49            1,024\n|    |    └─ReLU: 3-50                   --\n|    |    └─Conv2d: 3-51                 131,072\n|    |    └─BatchNorm2d: 3-52            1,024\n|    └─ResidualBlock: 2-9                --\n|    |    └─Conv2d: 3-53                 2,359,296\n|    |    └─BatchNorm2d: 3-54            1,024\n|    |    └─ReLU: 3-55                   --\n|    |    └─Conv2d: 3-56                 2,359,296\n|    |    └─BatchNorm2d: 3-57            1,024\n|    |    └─ReLU: 3-58                   --\n├─AdaptiveAvgPool2d: 1-3                 --\n├─Linear: 1-4                            5,130\n=================================================================\nTotal params: 11,181,642\nTrainable params: 11,181,642\nNon-trainable params: 0\n=================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = summary(ResNet18())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (prep): ResNetPrep(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "      (conv_down): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bnorm_down): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ1): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activ2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "ResNet18()"
   ]
  },
  {
   "source": [
    "### Wide ResNets\n",
    "\n",
    "Wide ResNets ([4](https://arxiv.org/abs/1605.07146)) are an extension of ResNets allowing for \"wider\" nets, not necessarily very deep.\n",
    "\n",
    "While the focal point of ResNets is to expand towards depth, a feat that was difficult to achieve in other nets like VGG, because of the vanishing gradient, the inventors of Wide ResNets argue that there can be a lot of components inside a regular ResNet with little contribution to the output; on the other hand, they noticed that, by decreasing the number of layers while increasing the number of channels, the resulting wide CNN can outperform the deep one.\n",
    "\n",
    "Wide ResNets, for instance, allow to reach accuracy > 95% on CIFAR10."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Deconvolution\n",
    "\n",
    "This is a summary of what found in [1](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "Deconvolution (*inverse convolution, transposed convolution*) is a process analogous to correlation. Usually, given a generic square matrix $M\\in \\mathbb{R}^{n\\times n}$ and kernel $K\\in \\mathbb{R}^{k\\times k},~k<n$, we have:\n",
    "\n",
    "$M \\otimes K = L$, with $L\\in\\mathbb{R}^{(n-k+1)~\\times~(n-k+1)}$\n",
    "\n",
    "With the deconvolution we want to solve the \"inverse\" problem, i.e., given the same kernel $K$, from a smaller matrix $H$, obtain a larger matrix $N$.\n",
    "\n",
    "$H \\tilde\\otimes K = N$\n",
    "\n",
    "The first equation can be transformed into a matrix-matrix multiplication prolbem by reshaping the kernel in the following way:\n",
    "\n",
    "$\\mathcal{K} =\n",
    "\\begin{Bmatrix}\n",
    "k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "\n",
    "Then, the convolution can be rethought as:\n",
    "\n",
    "$ \\mathcal{K} \\cdot\\text{vec}(M) = \\text{vec}(L)$, with a later row-wise reshaping of $\\text{vec}(L)$\n",
    "\n",
    "Similarly, we can view the deconvolution problem as:\n",
    "\n",
    "$ \\mathcal{K}^\\top \\cdot\\text{vec}(H) = \\text{vec}(N)$\n",
    "\n",
    "dimensions: $(16\\times4) \\cdot (4\\times1) = (16\\times1)$\n",
    "\n",
    "Thus, by properly adjusting the weights $\\begin{Bmatrix}k_{00} & k_{01} & k_{02} & k_{10} & k_{11} & k_{12} & k_{20} & k_{21} & k_{22}\\end{Bmatrix}$, we may also be able to recover the matrix $M$ starting from $L$.\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "Deconvolution is used in tasks requiring an \"intelligent upsampling\", or we might call it an \"upsampling with feature identification\". A trivial upsampling can be achieved by using techniques like \n",
    "\n",
    "*max-unpooling*: ![](https://www.researchgate.net/publication/335754645/figure/fig4/AS:839690152329216@1577209141459/Example-of-an-unpooling-process-Indices-of-max-pooling-are-kept-up-and-reused-to.png)\n",
    "\n",
    "*regular convolution with a lot of padding*: ![](https://miro.medium.com/max/2588/1*uk4KJEtyDuPOipfG4yd-WA.gif)\n",
    "\n",
    "but the final result will be really unsatisfying.\n",
    "\n",
    "Upsampling may be required e.g. in tasks where the output size must be the same as the input size.\n",
    "\n",
    "This strategy is often employed for tasks of **semantic segmentation**\n",
    "\n",
    "![](https://i.pinimg.com/originals/62/d8/5a/62d85a6488ffa85bde7af3a9a8ed70ff.gif)\n",
    "\n",
    "The task can be summarized with this phrase: instead of trying to classify the whole image, we wish to classify each pixel within a large image.\n",
    "\n",
    "One of the most succesful CNNs for image segmentation is U-Net, which was originally proposed for medical imaging.\n",
    "\n",
    "![](img/unet_small.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### U-Net [3](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "The architecture is a composition of two parts: a contracting module and an expanding module.\n",
    "\n",
    "The contracting module is a sequence of VGG convolutional blocks. \n",
    "\n",
    "After reaching the bottom part, we begin upsampling the image following the inverse of the scheme from the contracting module, with an additional operation: we concatenate the output of the upsampling with the output from the last convolutional layer of the corresponding block (as in the image). Thus, if the upsampling yields a 256-channel output, we concatenate this output with the output of the last 256-channel convolutional layer from the contracting module. This leaves us with a 512-channel tensor which we convolve to 256-channels once again. Note that, if the spatial dimensions of the data from the contracting module doesn't match those of the upsampled data, cropping is operated so that we can safely concatenate the two tensors.\n",
    "\n",
    "Actually, the original implementation of U-Net operates a semantic segmentation on a window which is approximately 2/3 of the original image (there will hence be a leftover band of pixels outside the center of the image). In the image below, the white thin lines represent the area that will be subject to the segmentation.\n",
    "\n",
    "![](img/unet_crop.jpg)\n",
    "\n",
    "For what concerns the output, instead, we end up with a tensor of shape $h^\\prime \\times w^\\prime \\times C$, where $C$ denotes the number of the classes we want to operate segmentation (logically speaking, **if we want to classify each pixel, we wish to produce a softmax for each pixel**).\n",
    "\n",
    "![](img/unet_last.jpg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Homework\n",
    "\n",
    "Taking inspiration from the picture above, implement a U-Net-style CNN with the following specs:\n",
    "\n",
    "1. All convolutions must use a $3\\times 3$ kernel and **leave the spatial dimensions (i.e. height, width) of the input untouched**.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a $2\\times 2$ kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a $2\\times 2$ kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel \n",
    "    * between how many classes are we discriminating?\n",
    "\n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures (e.g., `nn.Module`s, private methods...) if you believe it helps readability of your code.\n",
    "\n",
    "Test, at least with random data, that the network is doing the correct tensor operations and that the output has the correct shape (e.g., use `assert`s in your code to see if the byproduct is of the expected shape).\n",
    "\n",
    "*Note: the overall organization of your work can greatly improve readability and understanding of your code by others. Please consider preparing your notebook in an organized fashion so that we can better understand (and correct) your implementation.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "\n",
    "[1](https://arxiv.org/abs/1603.07285) Dumoulin, Vincent, and Francesco Visin. \"A guide to convolution arithmetic for deep learning.\"\n",
    "\n",
    "[2](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition\n",
    "\n",
    "[3](https://arxiv.org/abs/1505.04597) Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention.\n",
    "\n",
    "[4](https://arxiv.org/abs/1605.07146) Zagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\"\n",
    "\n",
    "[5](https://arxiv.org/abs/1409.1556v6) Simonyan and Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}