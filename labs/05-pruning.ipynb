{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd08aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0",
   "display_name": "Python 3.8.6 64-bit ('lottery': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 5\n",
    "\n",
    "## Pruning in Artificial Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Labs\n",
    "\n",
    "* Essentially, we now have a bag of _tricks_ to utilize in order to ensure our Multilayer Perceptrons (MLPs) train well while preserving a good generalization\n",
    "* We know how to construct a simple MLP with a custom number of layers and neurons\n",
    "* We know how to train the MLP using popular variants of Stochastic Gradient Descent (SGD) such as SGD with momentum and Adam\n",
    "* We learned some tricks to enforce a better generalization capability of our MLP, namely Weight Decay and Dropout\n",
    "* We saw how to use proper learning rate schedules for better convergence of our network\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* Today, we are going more into the detail of Deep Learning (DL) for a more technical lab session devoted to a more \"experimental\" branch: **network pruning**\n",
    "* We will see how to implement a specific pruning algorithm, Magnitude Pruning (MP), in an MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is pruning?\n",
    "\n",
    "Pruning is referred to the specific act of **removing parameters from a Machine learning model**.\n",
    "\n",
    "The term originated from tree-based models that, when grown too deep, present pronounced tendencies towards overfitting, so their branches need to be cut (*pruned*) with proper criteria in order to ensure a better generalization.\n",
    "\n",
    "The main difference between pruning tree-based models and Artificial Neural Networks (ANNs) lies in the fact that, while in tree-based models we operate pruning on _leaf parameters_, in ANNs we usually prune without regard for the position of the parameter, which is a connection (_synapse_) between neurons.\n",
    "\n",
    "![](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/DTprune.png)\n",
    "\n",
    "*In decision trees, models are pruned _downside up_ starting from the leaves. Picture from [Carnegie Mellon University](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/).*\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/23/Before_after_pruning.png)\n",
    "\n",
    "*In ANNs, generically, pruning is operated regardless of the position of the neuron/connection within the network. Picture from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Before_after_pruning.png).*\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Why pruning?\n",
    "\n",
    "Pruning (also called _Weight Sparsification_) is a collection of techniques falling within the category of **Network Miniaturization/Compression**, a bulk of practices aimed at reducing the size, or memory impact, or energetic impact, of a well-performing ANN, while still allowing the _reduced_ model to perform on-par with the original network.\n",
    "\n",
    "Some other examples of Network Compression include, but not limited to:\n",
    "* Knowledge Distillation\n",
    "* Weight Quantization\n",
    "* Weight Sharing\n",
    "\n",
    "Also, pruning complies with Occam's Razor, which states that \"it is useless to do with more, what can be done with less\", referring to the parameters in this case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How to prune?\n",
    "\n",
    "Pruning in ANNs is a theme that has been around since the early works of Yann LeCun [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7223&rep=rep1&type=pdf) in the late 80s. A huge number of techniques has been proposed since then [3](https://arxiv.org/abs/2102.00554).\n",
    "\n",
    "The findings in these early papers are essentialy different from those in the most recent ones (i.e., from 2015 on) mainly because the networks back then were not deep: large ANNs tend instead to react differently to pruning.\n",
    "\n",
    "Regarding how to prune, we can distinguish between two main categories: **Structured Pruning** and **Unstructured Pruning**.\n",
    "\n",
    "### Structured pruning\n",
    "\n",
    "To render this classification as neat as possible, structured pruning acts on multiple synapses logically connected in some way. In the case of MLPs, we may think of the act of _removing a neuron_ from a given layer as a structured pruning technique, as we **remove all the connections incoming towards that neuron**.\n",
    "\n",
    "![](img/struct_prune.png)\n",
    "\n",
    "In the image above, we prune the first neuron of the second layer. This equates to removing all of its incoming connections. The effect of this on the parameters of the ANN is that now we have a so called *regular sparsity*: the original parameters $\\Theta\\in\\mathbb{R}^{4\\times 3}$ are now $\\tilde\\theta\\in\\mathbb{R}^{4\\times 2}$.\n",
    "\n",
    "### Unstructured pruning\n",
    "\n",
    "In this case, instead, we utilize a pruning technique which has no regard for the geometry or the structure of the layers themselves.\n",
    "\n",
    "![](img/unstr_prune.png)\n",
    "\n",
    "In the image above, we removed four connections. This leads to an *irregular* form of sparsity in the parameters, which can't be taken advantage of directly, as happened instead in the structured case.\n",
    "There exist algorithms for sparse linear algebra which can be used in these cases (implemented for example in CuSPARSE) or some specific processor architectures are designed to take advantage of sparsity in matrices (e.g., NVIDIA Tesla A100 GPU).\n",
    "\n",
    "In this lab we will deal with unstructured sparsity, but an extension to structured sparsity can be easily derived."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Least) Magnitude pruning\n",
    "\n",
    "MP is one of the most straight-forward pruning techniques. Essentially, we remove a fixed fraction of parameters exhibiting a low absolute value.\n",
    "\n",
    "To get more technical:\n",
    "\n",
    "* Let us call $\\Theta$ the *structure* holding all of the parameters in the network.\n",
    "\n",
    "* Let us also introduce $p\\in(0,1)$, the **pruning rate**. This is the fixed fraction of parameters with small magnitude that we will prune.\n",
    "\n",
    "1. We obtain $\\vec\\Theta = \\text{vec}\\vert\\Theta\\vert$, the **empirical distribution of the parameters in magnitude**\n",
    "\n",
    "2. We sort $\\vec\\Theta$, obtaining $\\vec\\Theta_\\text{sort}$\n",
    "\n",
    "3. We obtain $\\theta_p$ the $p$-th percentile of $\\vec\\Theta_\\text{sort}$, which will act as a threshold for determining which parameters are going to be pruned.\n",
    "\n",
    "4. For each of the parameter $\\theta$ in the original $\\Theta$, we operate the pruning: $\\tilde\\theta = \\theta \\cdot \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$ (NB: $\\mathbb{1}$ is the *indicator function*, which evaluates to 1 if the condition inside the brackets is verified, to 0 otherwise).\n",
    "\n",
    "5. We replace $\\Theta$ with $\\tilde\\Theta$, the structure composed of the various $\\tilde\\theta$s.\n",
    "\n",
    "#### Masking\n",
    "\n",
    "The same procedure described above can be obtained with masking. Essentially, instead of directly building $\\tilde\\Theta$ as element-wise application of $\\tilde\\theta = \\theta \\cdot \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$, we rather do this:\n",
    "\n",
    "4. $m = \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$. $m$ is a boolean telling us whether the corresponding parameters needs to be pruned or not. We have as many $m$s as there are $\\theta$s.\n",
    "\n",
    "5. We compose the $m$s in a structure $M$, identical in shape to $\\Theta$.\n",
    "\n",
    "6. Now, $\\tilde\\Theta = M\\odot \\Theta$, where $\\odot$ is the Hadamard product.\n",
    "\n",
    "This is the way in which pruning (analogously to Dropout) will be operated by us on PyTorch.\n",
    "\n",
    "#### Global vs Local\n",
    "\n",
    "The current version of MP operates on a global logic: we pool all of the parameters and prune w.r.t. one single threshold $\\theta_p$.\n",
    "If we wanted to complicate things just a little bit, we could instead pool parameters in different buckets and prune buckets separately (local approach):\n",
    "\n",
    "0. Divide the parameters in buckets $\\Theta_b, b\\in\\{1,...,B\\},~~\\cup_b\\Theta_b=\\Theta,~\\Theta_b\\cap\\Theta_{b^\\prime} = \\varnothing$.\n",
    "\n",
    "All the following points 1.-6. are operated separately for each bucket, so we will have a thresold for each bucket, a mask for each bucket...\n",
    "\n",
    "This strategy might be operated if we wanted to operate pruning separately for some specific layers. For example, [4](https://arxiv.org/abs/1803.03635) specifies that the reached better results when operating lower pruning rates on output layers. Nonetheless, this practice has fallen off, since in Deep ANNs, a global approach has been empirically proven more effective."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Perchance, did we already meet magnitude pruning somewhere???\n",
    "\n",
    "Indeed, L1-norm regularization is a form of *implicit pruning* operated during the training of the network. Although in theory it seems to work, in practice it doesn't work as well as MP, and some works (see [3](https://arxiv.org/abs/2102.00554)) have shown that L1-norm regularization does not work well with normalization layers, which play a fundamental role for faster convergence of SGD."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts import mnist, train_utils, architectures, train"
   ]
  },
  {
   "source": [
    "For the point 4., we'll be using `torch.where`. Let's see first what it is and how does it work:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tuple'>\n(tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n        4, 4, 4, 4]), tensor([2, 6, 9, 1, 2, 3, 4, 5, 6, 7, 2, 4, 5, 6, 7, 0, 4, 5, 6, 7, 8, 9, 0, 1,\n        2, 3, 7, 8]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5,10)) # random tensor\n",
    "y = torch.where(x<.5)\n",
    "print(type(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6689)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "source": [
    "It returns a tuple of length `d`, where `d` is the number of dimensions of the tensor `x`.\n",
    "\n",
    "But we can use it differently:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 1, 1, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "torch.where(x<.5, 1, 0)"
   ]
  },
  {
   "source": [
    "by specifying two following arguments, we actually create a tensor with the same structure as `x`, where the element of `x` abiding to the condition in `torch.where` take on the value reported as the second argument; those who do not, assume the third argument instead. I hope it is straightforward how this method will be integrated in the pruning fct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, pruning_rate):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    '''\n",
    "    torch.cat applied to a list of vectors (i.e., tensors with single dimension) as in this case\n",
    "    returns a single long vector which is the concatenation of the vectors inside the list\n",
    "    '''\n",
    "    flat = torch.cat([pars.abs().flatten() for pars in model.parameters()], dim=0)\n",
    "    # 2. sort this distribution\n",
    "    '''\n",
    "    torch.sort returns a tuple:\n",
    "      element 0 is the sorted tensor\n",
    "      element 1 is the tensor of indices of the original tensor, i.e. it tells us how to recover the original\n",
    "                tensor from the sorted one\n",
    "    '''\n",
    "    flat = flat.sort()[0]\n",
    "    # 3. obtain the threshold\n",
    "    position = int(pruning_rate * flat.shape[0])\n",
    "    thresh = flat[position]\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    mask = [torch.where(pars.abs() >= thresh, 1, 0) for pars in model.parameters()]\n",
    "    # 6. obtain the new structure of parameters\n",
    "    for param, m in zip(model.parameters(), mask):\n",
    "        param.data *= m\n",
    "    # 7. what do we need to return?\n",
    "    '''\n",
    "    we need to return the mask because it will be needed for subsequent computations\n",
    "    '''\n",
    "    return mask"
   ]
  },
  {
   "source": [
    "Let us see our algorithm at work:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "# MLPCustom is a net architecture I prepared to create MLPs with less code to write than before.\n",
    "# See the implementation in the `architectures` script for further insights\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = magnitude_pruning(net, .2)"
   ]
  },
  {
   "source": [
    "Did it work? Let's analyze the mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_ones_in_mask(mask):\n",
    "    return sum([m.sum().item() for m in mask]) / sum([m.numel() for m in mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.8 \n\ntensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0]) \n tensor([ 0.0098,  0.0166,  0.0328,  0.0179,  0.0128,  0.0353, -0.0000,  0.0000,\n        -0.0221, -0.0000,  0.0000,  0.0125,  0.0000, -0.0252, -0.0194, -0.0272,\n        -0.0324, -0.0118,  0.0306, -0.0000], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of ones in mask:\", number_of_ones_in_mask(mask), \"\\n\")\n",
    "print(mask[0][0,:20], \"\\n\", next(net.parameters())[0,:20])"
   ]
  },
  {
   "source": [
    "#### **Q**: What is wrong with this implementation?\n",
    "\n",
    "*Think about the type of layers we have in our MLP...*\n",
    "\n",
    "Answer: the function prunes also the weights linked to the batchnorm. We must find a way to avoid that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layers.1.weight torch.Size([16, 784])\nlayers.1.bias torch.Size([16])\nlayers.3.weight torch.Size([16])\nlayers.3.bias torch.Size([16])\nlayers.4.weight torch.Size([32, 16])\nlayers.4.bias torch.Size([32])\nlayers.6.weight torch.Size([32])\nlayers.6.bias torch.Size([32])\nlayers.7.weight torch.Size([64, 32])\nlayers.7.bias torch.Size([64])\nlayers.9.weight torch.Size([64])\nlayers.9.bias torch.Size([64])\nlayers.10.weight torch.Size([10, 64])\nlayers.10.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.shap)"
   ]
  },
  {
   "source": [
    "We want to prune weights corresponding to the Linear layers (the only modules with learnable parameters). In the list above, the weights of the linear layers are those corresponding to layer 1 (`layers.1.weight`, `layers.1.bias`), 4, 7, and 10.\n",
    "We can enucelate them by **filtering out** parameters which don't have these numbers in their name when we're calling `net.named_parameters()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, pruning_rate, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"]):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    '''\n",
    "    model.named_parameters returns a tuple:\n",
    "      element 0 is the name of the parameter\n",
    "      element 1 is the parameter itself\n",
    "    any(<list of booleans>)\n",
    "    checks whether any of the conditions inside the list is True.\n",
    "    [l in pars[0] for l in layers_to_prune] checks whether \"1\", \"4\", \"7\", and \"10\" are inside the parameter name\n",
    "    hence, there will be up to 1 True in the corresponding list: in that case, any(...) returns True\n",
    "    '''\n",
    "    params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "    flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "\n",
    "    # 3. obtain the threshold\n",
    "    position = int(pruning_rate * flat.shape[0])\n",
    "    thresh = flat[position]\n",
    "\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    # 6. obtain the new structure of parameters\n",
    "    '''\n",
    "    I do this process with a for cycle instead of a list comprehension for clarity\n",
    "    * if the layer is a layer to prune → populate the mask with 1s and 0s\n",
    "    * otherwise → just populate the mask with ones\n",
    "    By doing so, I can immediately apply the mask to the model as well...\n",
    "    '''\n",
    "    mask = []\n",
    "    for pars in model.named_parameters():\n",
    "        if any([l in pars[0] for l in layers_to_prune]):\n",
    "            m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "            mask.append(m)\n",
    "            pars[1].data *= m\n",
    "        else:\n",
    "            mask.append(torch.ones_like(pars[1]))\n",
    "            # no need to multiply as it's all 1s\n",
    "\n",
    "    # 7. what do we need to return?\n",
    "    return mask"
   ]
  },
  {
   "source": [
    "## When to prune?\n",
    "\n",
    "Usually, pruning is operated on a **fully trained model**.\n",
    "\n",
    "Let us see what happens when we prune a trained model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 7349.624662399292 - performance 0.9631333333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7349.624662399292, 0.9631333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# load pretrained model\n",
    "state_dict = torch.load(\"models_push/mlp_custom_mnist/mlp_custom_mnist.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5069608452454941"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "mask = magnitude_pruning(net, pruning_rate=0.5)\n",
    "number_of_ones_in_mask(mask)"
   ]
  },
  {
   "source": [
    "Note: we now expect the number of 1s in the mask to be >= 50% since we have also a small chunk of parameters which never gets pruned (batchnorm). The pruning is applied only to Linear layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 9346.077312469482 - performance 0.9520333333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9346.077312469482, 0.9520333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "source": [
    "We see that there's a drop in performance. This is a characteristic of pruning. Rarely a simple application of pruning leaves the performance untouched or better. Due to that, we must procede with a re-training of the ANN.\n",
    "\n",
    "## Retraining the network\n",
    "\n",
    "The retraining can be carried out in different ways:\n",
    "\n",
    "### Fine-tune (FT)\n",
    "\n",
    "This means that, after pruning, we retrain for a few epochs without touching the optimizer (i.e., we do not employ a LR schedule but leave the hyperparameters untouched w.r.t. their final configuration).\n",
    "\n",
    "### Learning Rate Rewind (LRR)\n",
    "\n",
    "After pruning, we re-train for the same number of epochs of the original training, resetting the optimizer hyperparameters as well\n",
    "\n",
    "### Weight Rewind (WR, a.k.a. _Lottery Ticket Hypothesis_)\n",
    "\n",
    "After pruning, we **reset** the surviving weights to their original configuration, then we re-train as in LRR. The idea behind the Lottery Ticket Hypothesis (LTH) is that, within a randomly initialized ANN, there lies a subnetwork (i.e., a subset of the ANN connections) who is actually responsible for the good performance of the dense ANN. It's hence assumed that this subnetwork has _won the lotter of initialization_.\n",
    "\n",
    "## Iterating the procedure\n",
    "\n",
    "Actually, all three procedures can be iterated (Iterative Magnitude Pruning, IMP). So, if we prune 25% of the ANN for each step, we can reach high pruning rates (e.g. 10 iterations of IMP $\\Rightarrow$ final sparsity of the ANN of about $0.75^{10}\\approx 0.0563$).\n",
    "\n",
    "Of course, IMP applied 10 times with a pruning rate of 0.25 yields far better result than a one-shot pruning + retraining with $p=0.95$.\n",
    "\n",
    "## Who wins and who doesn't?\n",
    "\n",
    "[5](https://arxiv.org/abs/2003.02389) does an excellent job in comparing these three paradigms, concluding that, as far as accuracy is involved, **LRR beats all of its competitors** on the long run. Despite that, a lot of recent publications still use a one-shot pruning followed by FT, this because LRR in the IMP setting can be very expensive in terms of time and energy. Moreover, we showed ([6](https://medvet.inginf.units.it/publications/2021-c-zmpa-speeding/)) that, at high pruning rates, we can \"cut the corners\" and re-train for less epochs while still getting good accuracy w.r.t. WR and LRR.\n",
    "\n",
    "## What about Weight Rewind?\n",
    "\n",
    "The LTH has sparked some debate in the ML community and has allowed for some interesting analysis, especially concerning the initial phase of training. In particular, the almost-defunct Uber AI has done some [additional research](https://eng.uber.com/deconstructing-lottery-tickets/) showing some interesting insights:\n",
    "* Random Pruning (RP) shows no benefit even after retraining. That confirms that re-training alone is not responsible for the effectiveness of the pruned network: you also need an _intelligent_ pruning algorithm;\n",
    "* Recalling the LTH, its application requires a parameters *reset* before retraining. As you can see from the image below, the sole application of the MP mask (_LT Mask_ in the picture) to a pruned and rewound network already increases the accuracy of the network by ~30%. This would seem to corroborate empirically the LTH.\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image3-2-696x497.png)\n",
    "\n",
    "* Another heuristic is the so-called **supermask**, which keeps parameters which have mantained the sign during training and have a large final value. Parameters crossing the zero, despite large, are going to be pruned nonetheless. Of course, the supermask is more memory-intensive because we need to keep track of the evolution of the parameters in time.\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image2-1-420x420.png)\n",
    "\n",
    "It's interesting to note how the supermask performs incredibly well after pruning and rewind (i.e., on untrained networks). The supermask is indicated as `large_final_same_sign`:\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image1-3-685x420.png)\n",
    "\n",
    "*All images from [Uber AI](https://eng.uber.com/deconstructing-lottery-tickets/)*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementation of the retraining procedure\n",
    "\n",
    "If we retrain the network as-is, we will soon notice that we did a big mistake, because we have in no way communicated to PyTorch that the MLP does **not** have to be retrained on the pruned parameters.\n",
    "\n",
    "In order to enforce that, we must act on the training loop, namely `train_epoch`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, params_type_to_prune):\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        y_hat = model(X)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ##### we must neutralize the gradient on the pruned params before the optimizer takes a step ####\n",
    "\n",
    "        if mask is not None:\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if any([l in name for l in layers_to_prune]):\n",
    "                    param.grad *= m\n",
    "\n",
    "        ######\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "source": [
    "let us integrate it with our `train_model` routine:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, mask, params_type_to_prune)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "source": [
    "Now, we can effectively retrain the pruned ANN by simply passing the `mask` to the training routine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Homework\n",
    "\n",
    "\"Close the loop\" for implementing IMP. Right now, the `magnitude_pruning` routine is thought for one-shot pruning. If you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of `magnitude_pruning` to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters).\n",
    "\n",
    "Hint:\n",
    "\n",
    "![](img/force.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "\n",
    "[1](https://arxiv.org/abs/2006.03669) O'Neill, James. \"An Overview of Neural Network Compression.\"\n",
    "\n",
    "[2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7223&rep=rep1&type=pdf) Yann LeCun, John Denker, Sara Solla. \"Optimal Brain Damage.\"\n",
    "\n",
    "[3](https://arxiv.org/abs/2102.00554) Hoefler, Torsten, et al. \"Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks.\"\n",
    "\n",
    "[4](https://arxiv.org/abs/1803.03635) Frankle, Jonathan, and Michael Carbin. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\"\n",
    "\n",
    "[5](https://arxiv.org/abs/2003.02389) Renda, Alex, Jonathan Frankle, and Michael Carbin. \"Comparing rewinding and fine-tuning in neural network pruning.\"\n",
    "\n",
    "[6](https://medvet.inginf.units.it/publications/2021-c-zmpa-speeding/) Zullich, Marco, Eric Medvet, Felice Andrea Pellegrino, and Ansuini, Alessio. \"Speeding-up pruning for Artificial Neural Networks: Introducing Accelerated Iterative Magnitude Pruning.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}