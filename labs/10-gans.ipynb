{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd02e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967",
   "display_name": "Python 3.8.8 64-bit ('lot': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 10\n",
    "\n",
    "## Generative Adversarial Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Generative Adversarial Networks (GANs) are a neural-based category of generative models.\n",
    "\n",
    "Up to now, we have seen only **discriminative models**, i.e. models tasked with learning $P(Y\\vert X)$, where $X$ are the observations and $Y$ is the dependent variable (category or scalars, depending uon the task).\n",
    "\n",
    "**Generative models**, on the other hand, are tasked with learning $P(Y, X)$, i.e. in simple terms they learn a _**rule**_ through which we can sample (generate) any amount of data.\n",
    "\n",
    "Specifically, GANs are expressed as a game in which two models are in competition:\n",
    "* a **generator** $G$ generates synthetic data\n",
    "* a **discriminator** $D$ distinguishes whether a piece of data is synthetic or not\n",
    "\n",
    "![](img/gan.jpg)\n",
    "\n",
    "$D$ is usually expressed as a *simple* network for binary classification and trained using the **binary cross-entropy loss** (BCELoss). Given $D(x)\\in[0,1]$ output of the discriminator and $y\\in\\{0,1\\}$ the ground truth, $\\mathcal{L}_D(\\hat{y}, y) = -y\\log(D(\\hat{y})) - (1-y) \\log(1-D(\\hat{y}))$.\n",
    "\n",
    "On the other hand, $G$ is a more complex entity. The input is a *latent variable* $z\\in\\mathbb{R}^d$, while the output is a point in the data space $\\mathcal{D}$ (i.e. $x\\in\\mathcal{D}$). The latent variable is sample from a fixed distribution, which usually is $\\mathcal{N}(\\mathbf{0}_d,\\sigma^2I_d )$.\n",
    "\n",
    "The generator, recall, is tasked with producing samples which $D$ misclassifies as real. We may then ask for the following: $G^\\star = \\text{argmax}_G \\{ \\mathcal{L}_D(G(z)) \\}$, or, that the generated data $G(z)$ *increase* the loss of $D$.\n",
    "\n",
    "Merging the two concepts, we express the GAN training as a *minmax* game:\n",
    "\n",
    "$\\min_G \\max_D \\log(D(x)) + \\log(1-D(G(z)))$ (note: the signs are switched w.r.t. $\\mathcal{L}_D$, so the $\\min$ becomes $\\max$ and vice-versa).\n",
    "\n",
    "In practical terms, the GANs are trained this way:\n",
    "\n",
    "1. Evaluate the discriminator on real data (usually marked with label `1`)\n",
    "2. Evaluate the discriminator on synthetic/fake data (label `0`) -- at the first iteration this is done on the generated data being pure noise\n",
    "    * the synthetic data is generated starting from a sample from the desidered latent distribution\n",
    "3. Update the params of the discriminator via $\\mathcal{L}_D$\n",
    "    * since the label is `1` for real data and `0` for fake data, it's easy to verify that the loss can be decomposed into two parts\n",
    "        * $-\\log(D(\\hat{y}))$ for the real data\n",
    "        * $-\\log(1-D(\\hat{y}))$ for the fake data. Notice that, since we deal with data generated by $G$, we may replace $\\hat{y}$ with $G(z)$: $-\\log(1-D(G(z)))$\n",
    "3. **After** having trained the discriminator, train the generator:\n",
    "    * we wish to **maximize** $-\\log(1-D(G(z)))$ (the \"fake part\" of the discriminator loss)\n",
    "    * this formulation can lead to an imbalanced discriminator (too strong w.r.t. generator in early training), so, [3](https://arxiv.org/pdf/1406.2661.pdf) suggests to minimize $-\\log(D(G(z)))$ instead.\n",
    "        * Notice that this formulation is equivalent to the **first part** of the discriminator loss, so while training $G$ we can still use the BCELoss, but inverting the labels for real and synthetic data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from scripts.mnist import get_data\n",
    "from scripts.torch_utils import use_gpu_if_possible\n",
    "from scripts.train_utils import AverageMeter\n",
    "from scripts.architectures import MLPCustom"
   ]
  },
  {
   "source": [
    "#### Data\n",
    "\n",
    "We're not using the presets from `scripts` since we need to update (a) the batch size and (b) the normalizing constants.\n",
    "\n",
    "**Q**: why are we not using `torchvision.transforms.Normalize(...)`?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"datasets\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"datasets\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "source": [
    "this is a free reinterpretation of DCGAN [1](https://arxiv.org/abs/1511.06434) build s.t. we can match the original shape of MNIST. We need to tweak the values of the transposed convolution because the original implementation is thought for images with $n^2$ spatial dimensions, while MNIST does not abide to that prerequisite. This is one of many possible implementations leading to an image of size (28, 28) as output.\n",
    "\n",
    "Also, we need to define a custom initialization function as in [1](https://arxiv.org/abs/1511.06434) the researchers found that very small weights are needed in order to make it work better.\n",
    "\n",
    "As far as the architectural choice is concerned, we can either:\n",
    "* put `nn.Sigmoid()` as the final activation and avoid data normalization\n",
    "* put `nn.Tanh()` as the final activation and normalize images using `mean=[0.5], std=[0.5]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on discrim and generator\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim_latent, base_width=64, output_ch=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(dim_latent, base_width*8, kernel_size=5, bias=False),\n",
    "            nn.BatchNorm2d(base_width*8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(base_width*8, base_width*4, kernel_size=5, bias=False),\n",
    "            nn.BatchNorm2d(base_width*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(base_width*4, base_width*2, kernel_size=5, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(base_width*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(base_width*2, base_width, kernel_size=5, bias=False),\n",
    "            nn.BatchNorm2d(base_width),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(base_width, output_ch, kernel_size=4, bias=False),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = z.unsqueeze(-1).unsqueeze(-1) # append two spatial dimensions to make it image-like\n",
    "        return self.layers(z)"
   ]
  },
  {
   "source": [
    "Let's test if the generator produces an output of the desired shape:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_latent = 100\n",
    "g = Generator(dim_latent=dim_latent)\n",
    "g.apply(weights_init)\n",
    "g(torch.rand((1,dim_latent))).shape"
   ]
  },
  {
   "source": [
    "The discriminator is a regular CNN for classification with a single neuron as output (=> binary classification).\n",
    "\n",
    "Note the use of Leaky ReLU, which is suggested for GANs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, base_width=64, input_ch=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(input_ch, base_width, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(base_width, base_width*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base_width*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(base_width*2, base_width*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base_width*4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_width*4, 1),\n",
    "            nn.Sigmoid() # squeeze output in the 0-1 axis so we can apply the loss\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X).flatten()"
   ]
  },
  {
   "source": [
    "Check if shape is correct: given batch of size $B$, we expect to have a tensor of shape `[B]` as output (one probability output per image in the batch)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator()\n",
    "d.apply(weights_init)\n",
    "d(torch.rand(10,1,28,28)).shape\n"
   ]
  },
  {
   "source": [
    "The loss function for the discriminator is the usual binary cross entropy.\n",
    "\n",
    "We moreover use `Adam` as optimizer both for the discriminator and the generator. The hyperparameters presented in the cell below are the ones suggested in the literature. Notice the use of a slightly smaller learning rate in both (usually the Adam default is `1e-3`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "lr = 2e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# 2 Adams with same hyperparameters\n",
    "optim_d = torch.optim.Adam(d.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optim_g = torch.optim.Adam(g.parameters(), lr=lr, betas=(beta1, beta2))\n"
   ]
  },
  {
   "source": [
    "We train for 3 epochs (it'll be well enough for this kind of CNN on MNIST...) and print the stats each 25 iterations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite_print = 25\n",
    "num_epochs = 3\n",
    "\n",
    "d = d.to(device)\n",
    "g = g.to(device)"
   ]
  },
  {
   "source": [
    "Moreover, at each split, we're going to generate some images starting from the same random sample (`fixed_noise`) from a $\\mathcal{N}(0_{100},I_{100})$ (the latent space). We store these images in the `fakeimgs` array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = 24\n",
    "fixed_noise = torch.randn([n_fake, dim_latent]).to(device)\n",
    "\n",
    "fakeimgs = []"
   ]
  },
  {
   "source": [
    "We build the training function in blocks.\n",
    "\n",
    "First off, we start with the discriminator. It's split in two parts:\n",
    "\n",
    "1. train the discriminator on the real data.\n",
    "2. train the discriminator on the synthetic data.\n",
    "\n",
    "The first step is very easy and basically attaches an array of 1's (`real_label`) as large as the batch size as ground truth to the images (since they're all real).\n",
    "\n",
    "The second step instead is a tiny bit more complicated: we start from a generic `synthetic_data` which we will generate outside this function, then attach to it an array of 0's as ground truth, then feed it to the discriminator and calculate the loss.\n",
    "\n",
    "The third function (`step_discriminator`) combines the previous two such that we first run the part on the real data, then the one on the synthetic data.\n",
    "\n",
    "Notice the `backward`s and `step`s calls:\n",
    "* we call `backward` twice: first in the real data part, then in the synthetic data part. This actually _accumulates_ (i.e. sums) gradients, which is what we want: the differentiation is a linear operator -- $\\nabla(f+g)=\\nabla f + \\nabla g$. Backward should be called on the sum of the two losses, but we can do it the same by calling backward two times on the two partial losses.\n",
    "* once we have accumulated twice, we can call `optimizer.step()` in the `step_discriminator` function.\n",
    "\n",
    "Also, we return the errors (losses) just for statistical purposes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_real_data(discriminator, real_data, loss_fn, device, real_label=1):\n",
    "    batch_size = real_data.shape[0]\n",
    "\n",
    "    ground_truth_real = torch.full([batch_size], real_label, dtype=torch.float).to(device)\n",
    "\n",
    "    discriminator_output_real = discriminator(real_data).view(-1)\n",
    "\n",
    "    error_real = loss_fn(discriminator_output_real, ground_truth_real)\n",
    "    error_real.backward()\n",
    "\n",
    "    return error_real\n",
    "\n",
    "def discriminator_synthetic_data(discriminator, generator, synthetic_data, loss_fn, device, fake_label=0):\n",
    "    batch_size = synthetic_data.shape[0]\n",
    "\n",
    "    ground_truth_fake = torch.full([batch_size], fake_label, dtype=torch.float).to(device)\n",
    "\n",
    "    discriminator_output_fake = discriminator(synthetic_data.detach()).view(-1)\n",
    "    error_fake = loss_fn(discriminator_output_fake, ground_truth_fake)\n",
    "\n",
    "    error_fake.backward()\n",
    "\n",
    "    return error_fake\n",
    "\n",
    "def step_discriminator(discriminator, generator, real_data, synthetic_data, optimizer, loss_fn, device, real_label=1, fake_label=0):\n",
    "    optimizer.zero_grad()\n",
    "    error_real = discriminator_real_data(discriminator, real_data, loss_fn, device, real_label=real_label)\n",
    "    error_fake = discriminator_synthetic_data(discriminator, generator, synthetic_data, loss_fn, device, fake_label=fake_label)\n",
    "    optimizer.step()\n",
    "\n",
    "    error_overall = error_real.item() + error_fake.item()\n",
    "\n",
    "    return error_overall"
   ]
  },
  {
   "source": [
    "Once we have advanced the discriminator, it's time for the generator. We utilize the same synthetic images used for the discriminator. This time, we attach the ground truth of 1's, for the reason talked about before.\n",
    "\n",
    "This function is almost completely identical to `discriminator_synthetic_data`, but for the fact that we flip the label of the ground truth and we call step in the end."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_generator(discriminator, generator, synthetic_data, optimizer, loss_fn, device, dim_latent, real_label=1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_size = synthetic_data.shape[0]\n",
    "    ground_truth_synth = torch.full([batch_size], real_label, dtype=torch.float).to(device)\n",
    "    discriminator_output_synth = discriminator(synthetic_data)\n",
    "\n",
    "    error_generator = loss_fn(discriminator_output_synth, ground_truth_synth)\n",
    "\n",
    "    error_generator.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return error_generator.item()"
   ]
  },
  {
   "source": [
    "We can wrap it all up together here.\n",
    "\n",
    "We generate the synthetic data once per iteration using the generator at the current state.\n",
    "\n",
    "We log and save the synthetic images generated from `fixed_noise` each `ite_print` iterations and at end of epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.train()\n",
    "d.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_data, _) in enumerate(trainloader):      \n",
    "\n",
    "        real_data = real_data.to(device)\n",
    "        noise = torch.randn([real_data.shape[0], dim_latent]).to(device)\n",
    "        synthetic_data = g(noise)\n",
    "        error_discrim = step_discriminator(d, g, real_data, synthetic_data, optim_d, loss_fn, device, real_label=real_label, fake_label=fake_label)\n",
    "        error_generator = step_generator(d, g, synthetic_data, optim_g, loss_fn, device, dim_latent, real_label=real_label)\n",
    "\n",
    "        if (i + 1) % ite_print == 0 or (i + 1) == len(trainloader):\n",
    "            print(f\"Ep. {epoch + 1}/{num_epochs} It. {i+1}/{len(trainloader)} >>> D loss {error_discrim:.3f} | G loss {error_generator:.3f}\")\n",
    "            g.eval()\n",
    "            sample = g(fixed_noise).detach().cpu()\n",
    "            fakeimgs.append(sample.reshape(sample.shape[0], 28, 28))\n",
    "            g.train()\n",
    "    \n"
   ]
  },
  {
   "source": [
    "Let us produce a collage of splits to visualize the results.\n",
    "This function allows us to stitch together an array of images onto a single numpy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_collage(nrow, ncol, img_range, dim=(28,28)):\n",
    "    img_collage = np.empty((nrow*dim[0], ncol*dim[1]))\n",
    "    for i, img in enumerate(img_range):\n",
    "        index_row = (i // ncol) * dim[0]\n",
    "        index_col = (i % ncol) * dim[1]\n",
    "        img_collage[index_row:index_row+dim[0], index_col:index_col+dim[1]] = img.reshape(dim)\n",
    "    \n",
    "    return img_collage"
   ]
  },
  {
   "source": [
    "Now, we shall visualize the first image generated at each split (each 25 iterations). We enlarge a bit the image using cv2 as 28*28 is too small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collage = produce_collage(3, 38, [imgs_epoch[0] for imgs_epoch in fakeimgs])\n",
    "collage = cv2.resize(collage, (collage.shape[1]*4, collage.shape[0]*4), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "source": [
    "Remember, the image is still a float array in the range defined by the transforms we applied before.\n",
    "If we want to save it, we first need to\n",
    "\n",
    "1. rescale it in the 0-255 interval\n",
    "2. convert it to `uint8`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_image(array):\n",
    "    img = array - array.min()\n",
    "    img = img * 255 / img.max()\n",
    "    return img.astype(\"uint8\")"
   ]
  },
  {
   "source": [
    "Now we can convert and save:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collage = array_to_image(collage)\n",
    "cv2.imwrite(\"img/GAN_collage.jpg\", collage)"
   ]
  },
  {
   "source": [
    "Takeaway: training vanilla GANs is **hard** and often we don't reach a real **convergence**: the training is a process of finding an equilibrium between generator and discriminator and you must fine-tune both architectures (and their optimizers) to find a good result. The literature on GANs is very extensive and a lot of different variants have been proposed to try solving these issues. One of the variants which we will present is called **Wasserstein GAN**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Wasserstein GAN\n",
    "\n",
    "The Wasserstein GAN (WGAN) ([2](https://arxiv.org/pdf/1701.07875.pdf)) replaces the discriminator with a **critic**, which is still an ANN, but with a different loss function: while the discriminator minimizes the binary cross-entropy loss, the critic minimizes an approximation of the **Wasserstein Distance** (also called Earth Mover's Distance, EMD) between the real distribution of data ($P_r$) and the *guide* distribution ($P_\\theta$), which is the distribution being learned by our generator.\n",
    "\n",
    "![](img/emd.jpg)\n",
    "\n",
    "$\\text{EMD}(P_r, P_\\theta) = \\inf_{\\gamma\\in\\Pi(P_r,P_\\theta)} \\text{E}_{(x,y)\\sim\\gamma}(\\vert\\vert x-y \\vert\\vert)$\n",
    "\n",
    "Here, $\\Pi(P_r,P_\\theta)$ represent the family of all possible joint distribution of $X, Y$ whose marginal $P(X)=P_r$ and $P(Y)=P_\\theta$, and $\\gamma$ is hence a sample from this family.\n",
    "\n",
    "More specifically, we wish to find the *minimal density mass* to be transported from $P_\\theta$ to $P_r$ such that $P_\\theta$ *becomes* $P_r$.\n",
    "\n",
    "It turns out that the critic can be an ANN $f_\\omega$ trained by back-propagating the gradient according to the following loss function:\n",
    "\n",
    "$\\mathcal{L}_f = - \\frac{1}{b}\\sum_{i=1}^{b} f_\\omega(x^{(i)}) + \\frac{1}{b}\\sum_{i=1}^{b}f_\\omega(g_\\theta(z^{(i)}))$\n",
    "\n",
    "The notation is easily inferrable from before:\n",
    "* $b$ is the batch size\n",
    "* $g$ is the generator parametrized by $\\theta$\n",
    "* $z$ is a sample from the *guide distribution* (usually, a Gaussian)\n",
    "* $f$ is the critic parametrized by $\\omega$\n",
    "\n",
    "$\\omega$ (parameters of $f$) is then updated using RMSProp (in the original paper) and clipping the resulting new parameters in the interval $[-0.01, 0.01]$.\n",
    "\n",
    "The loss of $g$ is instead\n",
    "\n",
    "$\\mathcal{L}_g = - \\frac{1}{b}\\sum_{i=1}^{b}f_\\omega(g_\\theta(z^{(i)}))$\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. The above losses don't require labels. We can then get rid of the \"real\" and \"fake\" labels we defined in the vanilla GAN\n",
    "2. The correct approximation of EMD with the above loss functions is valid only when the critic abides to a local form of lipschitzianity. This is enforced by clipping the weights $\\omega$ in a pre-defined interval ($[-0.01, 0.01]$ in the paper)\n",
    "3. The training of the critic and the generator doesn't always happen in an alternate fashion: $\\omega$ is updated $k$ times each update of $\\theta$. In the paper, $k=5$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this case, we're going to use MLPs for both critic (discriminator) and generator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = use_gpu_if_possible()\n",
    "\n",
    "d = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "g = nn.Sequential(\n",
    "    nn.Linear(100, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.Linear(64, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 28*28),\n",
    "    nn.Tanh()\n",
    ").to(device)"
   ]
  },
  {
   "source": [
    "The paper specifically instruct to use RMSProp as an optimizer with a specific very small learning rate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_d = torch.optim.RMSprop(d.parameters(), lr=5e-5)\n",
    "optim_g = torch.optim.RMSprop(g.parameters(), lr=5e-5)"
   ]
  },
  {
   "source": [
    "The idea is that we train the critic $k$ times before updating the generator. We call $n$ `n_critic_training`.\n",
    "\n",
    "Note also that we need a lot of training epochs before obtaining a quality result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the ratio of critic update vs generator update\n",
    "n_critic_training = 5\n",
    "num_epochs = 200\n",
    "fakeimgs = []"
   ]
  },
  {
   "source": [
    "Onto the process of training, we can simply redefine the training loop used before by replacing the correct loss functions. Note that we don't need the labels anymore since the loss functions operate merely on the output of the critic and the discriminator.\n",
    "\n",
    "Note that we can fuse the part concerning real and synthetic data as right now they're much shorter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_critic(critic, generator, real_data, synthetic_data, optimizer, critic_clamp_value=0.01):\n",
    "    optimizer.zero_grad()\n",
    "    error_critic = -critic(real_data).mean() + critic(synthetic_data.detach()).mean()\n",
    "    error_critic.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for param in critic.parameters():\n",
    "        param.data.clamp_(-critic_clamp_value, critic_clamp_value)\n",
    "\n",
    "    return error_critic.item()\n",
    "\n",
    "def step_generator(critic, generator, synthetic_data, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    error_generator = -critic(synthetic_data).mean()\n",
    "    error_generator.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return error_generator.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.load_state_dict(torch.load(models_push/wgan/discriminator.pt))\n",
    "g.load_state_dict(torch.load(models_push/wgan/generator.pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.train()\n",
    "d.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_data, _) in enumerate(trainloader):\n",
    "        real_data = real_data.to(device)\n",
    "        noise = torch.randn([real_data.shape[0], dim_latent])\n",
    "        #noise = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (real_data.shape[0], dim_latent))))\n",
    "        synthetic_data = g(noise.to(device))\n",
    "\n",
    "        error_critic = step_critic(d, g, real_data, synthetic_data, optim_d)\n",
    "\n",
    "        if i % n_critic_training == 0:\n",
    "            # note: the WGAN paper specifies to re-sample data in G training\n",
    "            noise = torch.randn([real_data.shape[0], dim_latent])\n",
    "            synthetic_data = g(noise.to(device))\n",
    "            error_generator = step_generator(d, g, synthetic_data, optim_g)\n",
    "        \n",
    "    print(f\"Ep. {epoch + 1}/{num_epochs} >>> C loss {error_critic:.3f} | G loss {error_generator:.3f}\")\n",
    "    g.eval()\n",
    "    sample = g(fixed_noise).detach().cpu()\n",
    "    fakeimgs.append(sample.reshape(fixed_noise.shape[0], 28, 28))\n",
    "    g.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Since it's a long training, let's save the params."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(d.state_dict(), \"models_push/wgan/discriminator.pt\")\n",
    "torch.save(g.state_dict(), \"models_push/wgan/generator.pt\")"
   ]
  },
  {
   "source": [
    "Let's visualize a couple of results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fakeimgs[197][0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collage = produce_collage(5, 5, [fakeimgs[int(i)][15] for i in np.linspace(0, 199, 25)])\n",
    "collage = cv2.resize(collage, (collage.shape[1]*4, collage.shape[0]*4), interpolation=cv2.INTER_NEAREST)\n",
    "collage = array_to_image(collage)\n",
    "cv2.imwrite(\"img/WGAN_collage4.jpg\", collage)"
   ]
  },
  {
   "source": [
    "## Variational AutoEncoder\n",
    "\n",
    "A Variational AutoEncoder (VAE) can be seen as a special case of AutoEncoder (AE) in which the bottleneck layer is seen as a collection of random variables instead of a fixed array of scalars.\n",
    "\n",
    "![](https://miro.medium.com/max/3110/0*uq2_ZipB9TqI9G_k)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "While the aim of the AE is to operate dimensionality reduction, finding a latent space which is able to maximally \"capture\" the variability of the data space, VAEs instead try to find a _**probabilistic space**_ (whose dimension is hopefully smaller than the dimension of the data) which can approximate the data distribution starting from a family of known distributions (usually Gaussians).\n",
    "\n",
    "This space is represented by the **bottleneck** (_latent_) **layer** in the image above.\n",
    "\n",
    "AEs are a stack of two structures: an **econder**, which is a cascade of layers of decreasingly smaller size leading to the bottleneck, while the **decoder** starts from the bottleneck, increases the size of each hidden space, leading to the output layer, which has the **same size of the input layer**. The aim is to train the network such that the input and the output activations are **as close as possible**.\n",
    "\n",
    "To achieve this, AEs use the **reconstruction loss** $\\sum_{i=1}^{N} (\\mathbf{\\hat{x}}_i - \\mathbf{x}_i)$ to find a deterministic mapping $e:\\mathbb{R}^p \\rightarrow \\mathbb{R}^d$ and another $d:\\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, with $d \\ll p$. $e$ is the **encoder** and $d$ the **decoder**.\n",
    "\n",
    "VAEs still retains the structure of encoder/decoder, but this time they are viewed as two probabilistic distributions (mappings):\n",
    "* encoder: $q_\\theta(z\\vert x)$. $z$ is the latent (random) variable represented by the bottleneck. We're given an input (i.e., an image), and we _encode_ it in a latent space which is a vector of random variables of a specific distribution family (see later)\n",
    "* decoder: $p_\\varphi(x\\vert z)$. We're giving a sample from the latent space and we want to re-construct the input from it.\n",
    "\n",
    "VAEs train encoders such that the latent representation is _as close as possible_ to a collection of known distributions, and decoders such that the reconstruction of the input is _as close as possible_ to the original input.\n",
    "\n",
    "VAEs employ a variation of the reconstruction loss, which is the reconstruction log-likelihood: $\\text{RLL} = \\log(p_\\varphi(x\\vert z))$. Given a sample from the **latent space**, we re-build an input $x$ through a learnt probability distribution $p(z)$. Via $p_\\varphi(x\\vert z)$ we can reconstruct the input, then evaluate how _likely_ this reconstruction is.\n",
    "\n",
    "Alongside the RLL, we attach a regularization term, which forces the encoder to stick to the encoding onto the desired family of distributions. This regularization term is represented by the Kullback-Leibler divergence, which is a measure of \"distance\" between probability distributions: $\\text{KL}[q_\\theta(z\\vert x)\\vert\\vert p(z)]$. $p(z)$ is the desided distribution for the latent space, usually $\\mathcal{N}(\\mathbf{0}_d,I_d)$.\n",
    "\n",
    "The loss for the training of VAEs is hence the negative RLL (negative because we want to maximize reconstruction likelihood) plus the KL regularization term. The penalty is necessary to _communicate_ the VAE that it must produce the encoding \"close\" to the $\\mathcal{N}(0,1)$ density. Wrapping things up:\n",
    "\n",
    "$l_i(\\theta,\\varphi) = - \\mathbb{E}_{z\\sim q_\\theta(z|x_i)} \\{\\log[p_\\varphi (x_i\\vert z)] \\} + \\text{KL}[q_\\theta(z\\vert x_i) \\vert\\vert p(z)]$\n",
    "\n",
    "where $i$ is a single datum, and $z\\sim q_\\theta(z|x_i)$ represent a sample from the encoder given the input datum $x_i$.\n",
    "\n",
    "![](https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.51.06-PM.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Practical implementations\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "To render it possible to the encoder to construct a probability distribution, we build it this way:\n",
    "* let $u$ be the latent dimension of $z$\n",
    "    * we'd expect the bottleneck layer to have dimension $u$\n",
    "* we replicate the bottleneck layer for $k$ times, where $k$ is the dimensionality of the sufficient statistic defining the distribution of $Z$\n",
    "    * e.g. for the Gaussian distribution, $(\\mu, \\sigma)$ is a sufficient statistic, so we replicate the bottleneck layer two times\n",
    "* the penultimate layer in the encoder will then feed its activations to $k$ different layers running in parallel\n",
    "\n",
    "If we have a bottleneck layer with 16 neurons, each neuron $j$ will represent a Gaussian distribution with mean $\\mu_j$ and std $\\sigma_j$. Practically, we'll have 16 neurons from a layer producing $\\mu$'s and 16 neurons from a layer producing $\\sigma$'s.\n",
    "\n",
    "![](https://miro.medium.com/max/2540/1*R0irE3x0tXIYndqRLprFmw.png)\n",
    "\n",
    "#### Sampling of latent variable\n",
    "\n",
    "To produce $z$ and feed it to the decoder, we hence sample from each of these 16 Gaussians by using the values of $\\mu$s and $\\sigma$'s.\n",
    "\n",
    "Also, note that the encoder is better fit to produce values in the range $(-\\infty, +\\infty)$ (unless you use some specific activation functions to limit its range), so, for a normal distribution in which $\\sigma\\in(0,+\\infty)$, usually you produce $\\log\\sigma$ through the encoder and then exponentiate it while sampling $z$ to produce a strictly positive value.\n",
    "\n",
    "#### Reparametrization trick\n",
    "\n",
    "Since the sampling of $z$ as explained before would cause non-differentiability within the network, we reproduce the sampling via a reparametrization: $z = \\mu(x) + \\Sigma(x)^{1/2} \\cdot\\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "![](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-4.36.34-PM.png)\n",
    "\n",
    "#### $\\text{RLL}$\n",
    "\n",
    "If we think about our pixels $x$ as Bernoulli random variables with an uknown parameter $\\pi$, then the reconstruction likelihood $p_\\varphi(x_i\\vert z)$ becomes $(\\pi)^{x_i} (1-\\pi)^{1-x_i}$.\n",
    "\n",
    "Applying $\\log$, we have $x_i\\log(\\pi) + (1-x_i)\\log(1-\\pi)$.\n",
    "\n",
    "Eventually, changing sign, we obtain the Binary Cross Entropy (BCE) Loss.\n",
    "\n",
    "Note that:\n",
    "* $x_i$ is treated as a \"reconstructed pixel\" for image $i$\n",
    "* $\\pi$ is the groud truth, i.e. the \"original pixel\"\n",
    "    * we're then penalizing $x_i$ when it _goes too far off_ of $\\pi$\n",
    "* since we're ensembling the log-likelihood in a sample of images (the batch), we're summing each BCE term, not averaging it. We'll hence need to communicate that to PT by specifying the argument `reduction=\"sum\"` to the loss. This tells PT that the loss must be calculated by summing individual tokens, not averaging them (which is the default behaviour).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, dim_input:int, dim_latent:int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(dim_input, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "        self.encode_mu = nn.Linear(128, dim_latent)\n",
    "        self.encode_sigma = nn.Linear(128, dim_latent)\n",
    "    \n",
    "    def forward(self, X:torch.Tensor) -> torch.Tensor:\n",
    "        h = self.encoder(X)\n",
    "        latent_mu = self.encode_mu(h)\n",
    "        latent_log_sigma = self.encode_sigma(h)\n",
    "        return latent_mu, latent_log_sigma\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, dim_input:int, dim_latent:int):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_latent, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, dim_input),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z:torch.Tensor) -> tuple:\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, dim_input:int, dim_latent:int):\n",
    "        super().__init__()\n",
    "        self.encoder = VAEEncoder(dim_input, dim_latent)\n",
    "        self.decoder = VAEDecoder(dim_input, dim_latent)\n",
    "    \n",
    "    def sample_latent(self, mu, log_sigma):\n",
    "        # with reparametrization trick\n",
    "        device = next(self.parameters()).device\n",
    "        # white_noise is the epsilon (~N(0,1))\n",
    "        white_noise = torch.randn(mu.shape).to(device)\n",
    "        return mu + (log_sigma / 2).exp() * white_noise\n",
    "\n",
    "    def forward(self, X):\n",
    "        mu, log_sigma = self.encoder(X)\n",
    "        z = self.sample_latent(mu, log_sigma)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, mu, log_sigma\n"
   ]
  },
  {
   "source": [
    "### Setting the stages for the training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Loss\n",
    "\n",
    "As told before, the loss is a composition of the `BCELoss` (which we can use \"for free\" since it's already present in PT) and the KL part, which we can calculate in close form since it's applied to two normal distributions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(output, ground_truth):\n",
    "    reconstruction_loss_fn = nn.BCELoss(reduction=\"sum\")\n",
    "    return reconstruction_loss_fn(output, ground_truth)\n",
    "\n",
    "def kl_vae(mu, log_sigma):\n",
    "    # calculated in close form\n",
    "    kl = .5 * (log_sigma.exp() ** 2 + mu ** 2 - 1) - log_sigma\n",
    "    return kl.sum()\n",
    "\n",
    "def vae_loss(output, mu, log_sigma, ground_truth):\n",
    "    rec_loss_val = reconstruction_loss(output, ground_truth)\n",
    "    \n",
    "    kl_loss_val = kl_vae(mu, log_sigma)\n",
    "    return rec_loss_val + kl_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "vae = VAE(28*28, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "device = use_gpu_if_possible()\n",
    "ite_print = len(trainloader)"
   ]
  },
  {
   "source": [
    "### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(torch.load(\"models_push/vae/vae.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae.train()\n",
    "vae = vae.to(device)\n",
    "flatten = nn.Flatten()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_meter = AverageMeter()\n",
    "    for i, (X, _) in enumerate(trainloader):\n",
    "        X = X.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_hat, mu, log_sigma = vae(X)\n",
    "        loss = vae_loss(X_hat, mu, log_sigma, flatten(X))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_meter.update(loss.item(), X.shape[0])\n",
    "\n",
    "        if (i + 1) % ite_print == 0 or (i + 1) == len(trainloader):\n",
    "            print(f\"Epoch {epoch+1} | Loss {loss_meter.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"models_push/vae/vae.pt\")"
   ]
  },
  {
   "source": [
    "### Generating images from our VAE\n",
    "\n",
    "#### Image generation starting from a test Image\n",
    "\n",
    "Let us consider the first test image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(testloader))[0][0]\n",
    "label = next(iter(testloader))[1][0]\n",
    "plt.imshow(img.reshape(28,28,1), cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "source": [
    "Let's first check how it reconstructs the image:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "vae.cpu()\n",
    "img_hat, mu, log_sigma = vae(img)\n",
    "plt.imshow(img_hat.reshape(28,28).detach().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = vae.sample_latent(mu, log_sigma)"
   ]
  },
  {
   "source": [
    "let us try to _tweak_ the latent distribution by applying some white noise to it..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tweaked = vae.sample_latent(mu.add(torch.rand_like(mu)), log_sigma)\n",
    "img_hat_tweaked = vae.decoder(z_tweaked)\n",
    "plt.imshow(img_hat_tweaked.reshape(28,28).detach().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "source": [
    "Let us see the values of $\\mu$ and $\\Sigma$..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_sigma.exp()"
   ]
  },
  {
   "source": [
    "Let's build a new sequence of images by varying the nineteenth value of this latent distribution on a scale going from $\\mu[18] - 3\\sigma[18]$ to $\\mu[18] + 3\\sigma[18]$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_range = []\n",
    "for j in torch.linspace((mu[0,18] - 3*log_sigma.exp()[0,18]).item(), (mu[0,18] + 3*log_sigma.exp()[0,18]).item(), 20):\n",
    "    z2 = z.clone()\n",
    "    z2[0,18] = j\n",
    "    img_range.append(vae.decoder(z_tweaked).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = produce_collage(5, 4, img_range)\n",
    "c = array_to_image(c)\n",
    "c = cv2.resize(c, (c.shape[1]*4, c.shape[0]*4), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"img/VAE_collage0.jpg\", c)"
   ]
  },
  {
   "source": [
    "Let's try to vary the fifth component on a larger scale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_range = []\n",
    "for j in torch.linspace(-100, 100, 25):\n",
    "    z2 = z.clone()\n",
    "    z2[0,0] = j\n",
    "    img_range.append(vae.decoder(z2).detach())\n",
    "c = produce_collage(5, 5, img_range)\n",
    "c = array_to_image(c)\n",
    "c = cv2.resize(c, (c.shape[1]*4, c.shape[0]*4), interpolation=cv2.INTER_NEAREST)\n",
    "cv2.imwrite(\"img/VAE_collage1.jpg\", c)"
   ]
  },
  {
   "source": [
    "do it with another one..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_range = []\n",
    "for j in torch.linspace(-100, 100, 25):\n",
    "    z2 = z.clone()\n",
    "    z2[0,5] = j\n",
    "    img_range.append(vae.decoder(z2).detach())\n",
    "c = produce_collage(5, 5, img_range)\n",
    "c = array_to_image(c)\n",
    "c = cv2.resize(c, (c.shape[1]*4, c.shape[0]*4), interpolation=cv2.INTER_NEAREST)\n",
    "cv2.imwrite(\"img/VAE_collage2.jpg\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = MLPCustom(layers)\n",
    "net.load_state_dict(torch.load(\"models_push/mlp_custom_mnist/mlp_custom_mnist.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda x: (x - 0.1307) / 0.3081\n",
    "img_range_norm = [normalize(img) for img in img_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net(torch.cat(img_range_norm)).argmax(dim=1)"
   ]
  },
  {
   "source": [
    "The part on VAE has been mainly inspired by this [tutorial by Jeremy Jordan](https://www.jeremyjordan.me/variational-autoencoders/).\n",
    "\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "[1](https://arxiv.org/abs/1511.06434) Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised representation learning with deep convolutional generative adversarial networks.\"\n",
    "\n",
    "[2](https://arxiv.org/pdf/1701.07875.pdf) Arjovsky, Martin, et al. \"Wasserstein GAN.\"\n",
    "\n",
    "[3](https://arxiv.org/pdf/1406.2661.pdf) Goodfellow, Ian, et al. \"Generative Adversarial Nets.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}